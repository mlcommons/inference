# The format of this config file is 'key = value'.
# The key has the format 'model.scenario.key'. Value is mostly int64_t.
# Model maybe '*' as wildcard. In that case the value applies to all models.
# All times are in milli seconds

# TEST09: Verify output token length in performance mode
# This test logs ALL samples and verifies mean output token length is within bounds.

# mode dictionary (0 = submission, 1 = accuracy, 2 = performance, 3 = find peak perf)
*.*.mode = 2

# Use a fixed RNG seed for reproducibility
*.*.accuracy_log_rng_seed = 720381539243781796

# Log ALL samples - set to a value >= total dataset size (6396 samples for gpt-oss)
# Using a large value ensures all samples are logged regardless of performance
*.*.accuracy_log_sampling_target = 10000

# Ensure we run through all samples
*.*.min_query_count = 6396
*.*.min_duration = 0

# Turn off sample concatenation for accurate logging
*.*.sample_concatenate_permutation = 0

# =============================================================================
# TEST09 Compliance Thresholds (read by run_verification.py, not by LoadGen)
# =============================================================================
# Output token length bounds for compliance verification
# Each benchmark defines its own thresholds based on reference implementation.
# For gpt-oss-120b:
# - dataset: performance dataset
# - reference mean-OSL: 1278.20
# - threshold: +/-10%
# 1278.2 * 0.90 = 1150.38
*.*.test09_min_output_tokens = 1150.38
# 1278.2 * 1.10 = 1406.02
*.*.test09_max_output_tokens = 1406.02
