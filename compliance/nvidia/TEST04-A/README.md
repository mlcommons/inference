# Test 04 : Verify SUT is not caching samples
## Introduction

The purpose of this test is to ensure that results are not cached on the fly when SUT sees duplicate sample IDs.

By default, QSL loads a subset of the dataset determined by Performance Sample Count (say P) and queries for each scenario are
built using samples from the PerformanceSample implying the same sample can get repeatedly sent to the SUT over 
the test duration.

This test requires measuring & comparing performance of SUT (PerformanceOnly, mode=2) for two audit settings:

 - (TEST04-A) Issue P unique samples: 
	 - Offline scenario, a single query with samples_per_query equivalent to P unique samples is issued
	 - Multi-Stream scenario, test ends after #queries = P/samples_per_query have been issued
     - Single-Stream/Server scenario test ends after P unique queries have been issued.
- (TEST04-B) Issue same sample P times: 
	- Offline scenario, the same sample is repeated P times to fill the query. This breaks the requirement
                                              of reading contiguous memory locations in Offline mode, but it is normal for an audit test, meant to 
                                              stress the SUT in newer ways, to cause performance degradation.
    - Multi-Stream scenario, the same query is repeated for #queries (=P/samples_per_query). 
	- Single-Stream/Server scenario test ends after sending P same queries.

## Prerequisites
Test script works best with Python 3.3 or later.

## Exempt Benchmarks
This test is not applicable for the following benchmarks whose performance is dependent on variably sized input samples:
 1. RNNT
 2. BERT
 3. DLRM

## Scenarios

 - This test is applicable for scenarios Offline, Server and Single Stream always.
 - This test is not applicable for Multi-Stream scenario if samples_per_query >= Performance Sample Count

## Pass Criteria
Performance of TEST04-B should be slower than performance of TEST04-A. To account for noise, TEST04-A can be upto 20% slower than TEST04-B for SingleStream scenario with very short latencies (<200us) & upto 10% slower otherwise.

## Instructions

### Part I : Run TEST04-A

 - Copy provided [audit.config](https://github.com/mlperf/inference/blob/master/v0.7/compliance/nvidia/TEST04-A/audit.config) file in TEST04-A folder to the corresponding benchmark directory from where the test is run
 - Run the benchmark
 - Verification that audit.config was properly read can be done by checking that loadgen has found audit.config in `mlperf_log_detail.txt`
 - `mlperf_log_detail.txt` and `mlperf_log_summary.txt` files from this run are required to be submitted under TEST04-A

### Part II : Run TEST04-B
 - Copy provided [audit.config](https://github.com/mlperf/inference/blob/master/v0.7/compliance/nvidia/TEST04-B/audit.config) file in TEST04-B folder to the corresponding benchmark directory from where the test is run
 - Run the benchmark
 - Verification that audit.config was properly read can be done by checking that loadgen has found audit.config in `mlperf_log_detail.txt`
 - `mlperf_log_detail.txt` and `mlperf_log_summary.txt` files from this run are required to be submitted under TEST04-B

### Part III : Compare performance of TEST04-A with TEST04-B
Check the performance reported by TEST04-A with that of TEST04-B by running the script as below & submit the stdout as `verify_performance.txt` 

	python verify_test4_performance.py -u <mlperf_log_summary.txt generated by TEST04-A> -s <mlperf_log_summary.txt generated by TEST04-B> | tee verify_performance.txt

Expected outcome:
	`TEST PASS`

Alternatively, the below script can be run which runs the above verification script as well as copies the `mlperf_log_detail.txt` and `mlperf_log_summary.txt` files from TEST04-A and TEST04-B runs and `verify_performance.txt` script to the output compliance directory for submission readiness:

`python3 run_verification.py -a TEST04A_RUNDIR -b TEST04-B_RUNDIR -o OUTPUT_DIR`

 - TEST04A_RUNDIR: Specifies path to the directory containing logs from compliance test run with TEST04-A config
 - TEST04B_RUNDIR: Specifies path to the directory containing logs from compliance test run with TEST04-B config
 - OUTPUT_DIR: Specifies the path to the output directory where compliance logs will be uploaded from, i.e. `inference_results_v0.7/closed/NVIDIA/compliance/GPU/resnet/Offline`

