{
  "_comment": "Generation configuration for gpt-oss-120b model",
  "_description": "These parameters control the text generation behavior",
  
  "max_new_tokens": 32768,
  "temperature": 1.0,
  "top_k": -1,
  "top_p": 1.0,
  
  "_parameter_descriptions": {
    "max_new_tokens": "Maximum number of tokens to generate per request (1-32768)",
    "temperature": "Sampling temperature (0.0 = deterministic, higher = more random). Typical: 0.001-2.0",
    "top_k": "Top-k sampling (number of highest probability tokens to consider). -1 = disabled",
    "top_p": "Top-p/nucleus sampling (cumulative probability threshold). 0.0-1.0, typically 1.0 for no filtering",
    
    "_additional_params_note": "SGLang supports additional parameters like:",
    "repetition_penalty": "Penalty for repeating tokens (typically 1.0-1.2)",
    "frequency_penalty": "Penalty based on token frequency (0.0-2.0)",
    "presence_penalty": "Penalty for tokens already present (0.0-2.0)",
    "min_tokens": "Minimum tokens to generate before stopping",
    "stop": "Stop sequences (list of strings)",
    "ignore_eos": "Whether to ignore EOS token (boolean)"
  },
  
  "_presets": {
    "deterministic": {
      "max_new_tokens": 10240,
      "temperature": 0.001,
      "top_k": 1,
      "top_p": 1.0
    },
    "creative": {
      "max_new_tokens": 10240,
      "temperature": 1.5,
      "top_k": 50,
      "top_p": 0.95
    },
    "balanced": {
      "max_new_tokens": 10240,
      "temperature": 0.7,
      "top_k": 40,
      "top_p": 0.9
    }
  }
}

