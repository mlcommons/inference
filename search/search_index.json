{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"MLPerf Inference Benchmarks","text":""},{"location":"#overview","title":"Overview","text":"<p>The currently valid MLPerf Inference Benchmarks as of MLPerf inference v5.0 round are listed below, categorized by tasks. Under each model you can find its details like the dataset used, reference accuracy, server latency constraints etc.</p>"},{"location":"#image-classification","title":"Image Classification","text":""},{"location":"#resnet50-v15","title":"ResNet50-v1.5","text":"<ul> <li>Dataset: Imagenet-2012 (224x224) Validation<ul> <li>Dataset Size: 50,000</li> <li>QSL Size: 1,024</li> </ul> </li> <li>Number of Parameters: 25.6 million</li> <li>FLOPs: 3.8 billion</li> <li>Reference Model Accuracy: 76.46% ACC</li> <li>Server Scenario Latency Constraint: 15ms</li> <li>Equal Issue mode: False</li> <li>High accuracy variant: No</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"#text-to-image","title":"Text to Image","text":""},{"location":"#stable-diffusion","title":"Stable Diffusion","text":"<ul> <li>Dataset: Subset of Coco2014<ul> <li>Dataset Size: 5,000</li> <li>QSL Size: 5,000</li> </ul> </li> <li>Number of Parameters: 3.5 billion </li> <li>FLOPs: 1.28 - 2.4 trillion</li> <li>Reference Model Accuracy (fp32):  CLIP: 31.74981837, FID: 23.48046692</li> <li>Required Accuracy (Closed Division):<ul> <li>CLIP: 31.68631873 \u2264 CLIP \u2264 31.81331801 (within 0.2% of the reference model CLIP score)</li> <li>FID: 23.01085758 \u2264 FID \u2264 23.95007626 (within 2% of the reference model FID score)</li> </ul> </li> <li>Equal Issue mode: False</li> <li>High accuracy variant: No</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"#object-detection","title":"Object Detection","text":""},{"location":"#retinanet","title":"Retinanet","text":"<ul> <li>Dataset: OpenImages<ul> <li>Dataset Size: 24,781</li> <li>QSL Size: 64</li> </ul> </li> <li>Number of Parameters: TBD</li> <li>Reference Model Accuracy (fp32) : 0.3755 mAP</li> <li>Server Scenario Latency Constraint: 100ms</li> <li>Equal Issue mode: False</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"#medical-image-segmentation","title":"Medical Image Segmentation","text":""},{"location":"#3d-unet","title":"3d-unet","text":"<ul> <li>Dataset: KiTS2019<ul> <li>Dataset Size: 42</li> <li>QSL Size: 42</li> </ul> </li> <li>Number of Parameters: 32.5 million</li> <li>FLOPs: 100-300 billion</li> <li>Reference Model Accuracy (fp32) : 0.86330 Mean DICE Score</li> <li>Server Scenario: Not Applicable</li> <li>Equal Issue mode: True</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"#language-tasks","title":"Language Tasks","text":""},{"location":"#question-answering","title":"Question Answering","text":""},{"location":"#bert-large","title":"Bert-Large","text":"<ul> <li>Dataset: Squad v1.1 (384 Sequence Length)<ul> <li>Dataset Size: 10,833</li> <li>QSL Size: 10,833</li> </ul> </li> <li>Number of Parameters: 340 million </li> <li>FLOPs: ~128 billion</li> <li>Reference Model Accuracy (fp32) : F1 Score = 90.874%</li> <li>Server Scenario Latency Constraint: 130ms</li> <li>Equal Issue mode: False</li> <li>High accuracy variant: yes</li> <li>Submission Category: Edge</li> </ul>"},{"location":"#llama2-70b","title":"LLAMA2-70B","text":"<ul> <li>Dataset: OpenORCA (GPT-4 split, max_seq_len=1024)<ul> <li>Dataset Size: 24,576</li> <li>QSL Size: 24,576</li> </ul> </li> <li>Number of Parameters: 70 billion</li> <li>FLOPs: ~500 trillion</li> <li>Reference Model Accuracy (fp32) :<ul> <li>Rouge1: 44.4312</li> <li>Rouge2: 22.0352</li> <li>RougeL: 28.6162</li> <li>Tokens_per_sample: 294.45</li> </ul> </li> <li>Server Scenario Latency Constraint:<ul> <li>TTFT: 2000ms</li> <li>TPOT: 200ms</li> </ul> </li> <li>Equal Issue mode: True</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter</li> </ul>"},{"location":"#text-summarization","title":"Text Summarization","text":""},{"location":"#gpt-j","title":"GPT-J","text":"<ul> <li>Dataset: CNN Daily Mail v3.0.0<ul> <li>Dataset Size: 13,368</li> <li>QSL Size: 13,368</li> </ul> </li> <li>Number of Parameters: 6 billion</li> <li>FLOPs: ~148 billion</li> <li>Reference Model Accuracy (fp32) :<ul> <li>Rouge1: 42.9865</li> <li>Rouge2: 20.1235</li> <li>RougeL: 29.9881</li> <li>Gen_len: 4,016,878</li> </ul> </li> <li>Server Scenario Latency Constraint: 20s</li> <li>Equal Issue mode: True</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"#mixed-tasks-question-answering-math-and-code-generation","title":"Mixed Tasks (Question Answering, Math, and Code Generation)","text":""},{"location":"#mixtral-8x7b","title":"Mixtral-8x7B","text":"<ul> <li>Datasets:<ul> <li>OpenORCA (5k samples of GPT-4 split, max_seq_len=2048)</li> <li>GSM8K (5k samples of the validation split, max_seq_len=2048)</li> <li>MBXP (5k samples of the validation split, max_seq_len=2048)</li> <li>Dataset Size: 15,000</li> <li>QSL Size: 15,000</li> </ul> </li> <li>Number of Parameters: 47 billion </li> <li>Reference Model Accuracy (fp16) :<ul> <li>OpenORCA<ul> <li>Rouge1: 45.4911</li> <li>Rouge2: 23.2829</li> <li>RougeL: 30.3615</li> </ul> </li> <li>GSM8K Accuracy: 73.78%</li> <li>MBXP Accuracy: 60.12%</li> </ul> </li> <li>Tokens_per_sample: 294.45</li> <li>Server Scenario Latency Constraint:<ul> <li>TTFT: 2000ms</li> <li>TPOT: 200ms</li> </ul> </li> <li>Equal Issue mode: True</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter</li> </ul>"},{"location":"#recommendation","title":"Recommendation","text":""},{"location":"#dlrm_v2","title":"DLRM_v2","text":"<ul> <li>Dataset: Synthetic Multihot Criteo<ul> <li>Dataset Size: 204,800</li> <li>QSL Size: 204,800</li> </ul> </li> <li>Number of Parameters: ~23 billion</li> <li>Reference Model Accuracy: AUC = 80.31%</li> <li>Server Scenario Latency Constraint: 60ms</li> <li>Equal Issue mode: False</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter</li> </ul>"},{"location":"#graph-neural-networks","title":"Graph Neural Networks","text":""},{"location":"#r-gat","title":"R-GAT","text":"<ul> <li>Dataset: Illinois Graph Benchmark Heterogeneous validation dataset<ul> <li>Dataset Size: 788,379</li> <li>QSL Size: 788,379</li> </ul> </li> <li>Number of Parameters: </li> <li>Reference Model Accuracy: ACC = 72.86%</li> <li>Server Scenario Latency Constraint: N/A</li> <li>Equal Issue mode: True</li> <li>High accuracy variant: No</li> <li>Submission Category: Datacenter</li> </ul>"},{"location":"#automotive","title":"Automotive","text":""},{"location":"#3d-object-detection","title":"3D Object Detection","text":""},{"location":"#pointpainting","title":"PointPainting","text":"<ul> <li>Dataset: Waymo<ul> <li>Dataset Size: 39,986</li> <li>QSL Size: 1,024</li> </ul> </li> <li>Number of Parameters: 44 million</li> <li>FLOPs: 3 trillion</li> <li>Reference Model Accuracy (fp32):  mAP: 54.25%</li> <li>Required Accuracy (Closed Division):<ul> <li>mAP: 54.25%</li> </ul> </li> <li>Equal Issue mode: False</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Edge</li> </ul>"},{"location":"#submission-categories","title":"Submission Categories","text":"<ul> <li>Datacenter Category: All benchmarks except bert are applicable to the datacenter category for inference v5.0.</li> <li>Edge Category: All benchmarks except DLRMv2, LLAMA2-70B, Mixtral-8x7B and R-GAT are applicable to the edge category for v5.0.</li> </ul>"},{"location":"#high-accuracy-variants","title":"High Accuracy Variants","text":"<ul> <li>Benchmarks: <code>bert</code>, <code>llama2-70b</code>, <code>gpt-j</code>,  <code>dlrm_v2</code>, and <code>3d-unet</code> have a normal accuracy variant as well as a high accuracy variant.</li> <li>Requirement: Must achieve at least 99.9% of the reference model accuracy, compared to the default 99% accuracy requirement.</li> </ul>"},{"location":"index_gh/","title":"MLPerf\u00ae Inference Benchmark Suite","text":"<p>MLPerf Inference is a benchmark suite for measuring how fast systems can run models in a variety of deployment scenarios. </p> <p>Please see the MLPerf Inference benchmark paper for a detailed description of the benchmarks along with the motivation and guiding principles behind the benchmark suite. If you use any part of this benchmark (e.g., reference implementations, submissions, etc.), please cite the following:</p> <pre><code>@misc{reddi2019mlperf,\n    title={MLPerf Inference Benchmark},\n    author={Vijay Janapa Reddi and Christine Cheng and David Kanter and Peter Mattson and Guenther Schmuelling and Carole-Jean Wu and Brian Anderson and Maximilien Breughe and Mark Charlebois and William Chou and Ramesh Chukka and Cody Coleman and Sam Davis and Pan Deng and Greg Diamos and Jared Duke and Dave Fick and J. Scott Gardner and Itay Hubara and Sachin Idgunji and Thomas B. Jablin and Jeff Jiao and Tom St. John and Pankaj Kanwar and David Lee and Jeffery Liao and Anton Lokhmotov and Francisco Massa and Peng Meng and Paulius Micikevicius and Colin Osborne and Gennady Pekhimenko and Arun Tejusve Raghunath Rajan and Dilip Sequeira and Ashish Sirasao and Fei Sun and Hanlin Tang and Michael Thomson and Frank Wei and Ephrem Wu and Lingjie Xu and Koichi Yamada and Bing Yu and George Yuan and Aaron Zhong and Peizhao Zhang and Yuchen Zhou},\n    year={2019},\n    eprint={1911.02549},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n</code></pre> <p>Please see here for the MLPerf inference documentation website which includes automated commands to run MLPerf inference benchmarks using different implementations.</p>"},{"location":"index_gh/#mlperf-inference-v60-submission-deadline-february-13-2026","title":"MLPerf Inference v6.0 (submission deadline February 13, 2026)","text":"<p>For submissions, please use the master branch and any commit since the 6.0 seed release although it is best to use the latest commit in the master branch.</p> <p>For power submissions please use SPEC PTD 1.11.1 (needs special access) and any commit of the power-dev repository after the code-freeze</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm, ncnn imagenet2012 edge yolo v11 vision/classification_and_detection pytorch, onnx COCO safe subset edge bert language/bert tensorflow, pytorch, onnx squad-1.1 edge dlrm-v3 recommendation/dlrm_v3 pytorch Synthetic dataset datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge Wan2.2-T2V-A14B-Diffusers text_to_video pytorch COCO 2014 datacenter llama2-70b language/llama2-70b pytorch OpenOrca datacenter llama3.1-405b language/llama3-405b pytorch LongBench, LongDataCollections, Ruler, GovReport datacenter mixtral-8x7b language/mixtral-8x7b pytorch OpenOrca, MBXP, GSM8K datacenter rgat graph/rgat pytorch IGBH datacenter pointpainting automotive/3d-object-detection pytorch, onnx Waymo Open Dataset edge llama3.1-8b language/llama3.1-8b pytorch CNN-Daily Mail edge,datacenter deepseek-r1 language/deepseek-r1 pytorch AIME, MATH500, gpqa, MMLU-Pro, livecodebench(code_generation_lite) datacenter whisper speech2text pytorch LibriSpeech edge,datacenter GPT\u2013OSS language/gpt-oss-120b pytorch mlperf_gpt_oss_performance, mlperf_gpt_oss_accuracy datacenter VLM language/VLM pytorch Shopify-product-catalogue datacenter"},{"location":"index_gh/#mlperf-inference-v51-submission-deadline-august-1-2025","title":"MLPerf Inference v5.1 (submission deadline August 1, 2025)","text":"<p>For submissions, please use the master branch and any commit since the 5.1 seed release although it is best to use the latest commit in the master branch.</p> <p>For power submissions please use SPEC PTD 1.11.1 (needs special access) and any commit of the power-dev repository after the code-freeze</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm, ncnn imagenet2012 edge retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge dlrm-v2 recommendation/dlrm_v2 pytorch Multihot Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge stable-diffusion-xl text_to_image pytorch COCO 2014 edge,datacenter llama2-70b language/llama2-70b pytorch OpenOrca datacenter llama3.1-405b language/llama3-405b pytorch LongBench, LongDataCollections, Ruler, GovReport datacenter mixtral-8x7b language/mixtral-8x7b pytorch OpenOrca, MBXP, GSM8K datacenter rgat graph/rgat pytorch IGBH datacenter pointpainting automotive/3d-object-detection pytorch, onnx Waymo Open Dataset edge llama3.1-8b language/llama3.1-8b pytorch CNN-Daily Mail edge,datacenter deepseek-r1 language/deepseek-r1 pytorch AIME, MATH500, gpqa, MMLU-Pro, livecodebench(code_generation_lite) datacenter whisper speech2text pytorch LibriSpeech edge,datacenter <ul> <li>Framework here is given for the reference implementation. Submitters are free to use their own frameworks to run the benchmark.</li> </ul>"},{"location":"index_gh/#mlperf-inference-v50-submission-deadline-february-28-2025","title":"MLPerf Inference v5.0 (submission deadline February 28, 2025)","text":"<p>For submissions, please use the master branch and any commit since the 5.0 seed release although it is best to use the latest commit in the master branch.</p> <p>For power submissions please use SPEC PTD 1.11.1 (needs special access) and any commit of the power-dev repository after the code-freeze</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm, ncnn imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge dlrm-v2 recommendation/dlrm_v2 pytorch Multihot Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter gpt-j language/gpt-j pytorch CNN-Daily Mail edge,datacenter stable-diffusion-xl text_to_image pytorch COCO 2014 edge,datacenter llama2-70b language/llama2-70b pytorch OpenOrca datacenter llama3.1-405b language/llama3-405b pytorch LongBench, LongDataCollections, Ruler, GovReport datacenter mixtral-8x7b language/mixtral-8x7b pytorch OpenOrca, MBXP, GSM8K datacenter rgat graph/rgat pytorch IGBH datacenter pointpainting automotive/3d-object-detection pytorch, onnx Waymo Open Dataset edge <ul> <li>Framework here is given for the reference implementation. Submitters are free to use their own frameworks to run the benchmark.</li> </ul>"},{"location":"index_gh/#mlperf-inference-v41-submission-deadline-july-26-2024","title":"MLPerf Inference v4.1 (submission deadline July 26, 2024)","text":"<p>For submissions, please use the master branch and any commit since the 4.1 seed release although it is best to use the latest commit. v4.1 tag will be created from the master branch after the result publication.</p> <p>For power submissions please use SPEC PTD 1.10 (needs special access) and any commit of the power-dev repository after the code-freeze</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm, ncnn imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm-v2 recommendation/dlrm_v2 pytorch Multihot Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter gpt-j language/gpt-j pytorch CNN-Daily Mail edge,datacenter stable-diffusion-xl text_to_image pytorch COCO 2014 edge,datacenter llama2-70b language/llama2-70b pytorch OpenOrca datacenter mixtral-8x7b language/mixtral-8x7b pytorch OpenOrca, MBXP, GSM8K datacenter <ul> <li>Framework here is given for the reference implementation. Submitters are free to use their own frameworks to run the benchmark.</li> </ul>"},{"location":"index_gh/#mlperf-inference-v40-submission-february-23-2024","title":"MLPerf Inference v4.0 (submission February 23, 2024)","text":"<p>There is an extra one-week extension allowed only for the llama2-70b submissions. For submissions, please use the master branch and any commit since the 4.0 seed release although it is best to use the latest commit. v4.0 tag will be created from the master branch after the result publication.</p> <p>For power submissions please use SPEC PTD 1.10 (needs special access) and any commit of the power-dev repository after the code-freeze</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm, ncnn imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm-v2 recommendation/dlrm_v2 pytorch Multihot Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter gpt-j language/gpt-j pytorch CNN-Daily Mail edge,datacenter stable-diffusion-xl text_to_image pytorch COCO 2014 edge,datacenter llama2-70b language/llama2-70b pytorch OpenOrca datacenter <ul> <li>Framework here is given for the reference implementation. Submitters are free to use their own frameworks to run the benchmark.</li> </ul>"},{"location":"index_gh/#mlperf-inference-v31-submission-august-18-2023","title":"MLPerf Inference v3.1 (submission August 18, 2023)","text":"<p>Please use v3.1 tag (<code>git checkout v3.1</code>) if you would like to reproduce the v3.1 results. </p> <p>For reproducing power submissions please use the <code>master</code> branch of the MLCommons power-dev repository and checkout to e9e16b1299ef61a2a5d8b9abf5d759309293c440. </p> <p>You can see the individual README files in the benchmark task folders for more details regarding the benchmarks. For reproducing the submitted results please see the README files under the respective submitter folders in the inference v3.1 results repository.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm, ncnn imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm-v2 recommendation/dlrm_v2 pytorch Multihot Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter gpt-j language/gpt-j pytorch CNN-Daily Mail edge,datacenter"},{"location":"index_gh/#mlperf-inference-v30-submission-03032023","title":"MLPerf Inference v3.0 (submission 03/03/2023)","text":"<p>Please use the v3.0 tag (<code>git checkout v3.0</code>) if you would like to reproduce v3.0 results.</p> <p>You can see the individual Readme files in the reference app for more details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"index_gh/#mlperf-inference-v21-submission-08052022","title":"MLPerf Inference v2.1 (submission 08/05/2022)","text":"<p>Use the r2.1 branch (<code>git checkout r2.1</code>) if you want to submit or reproduce v2.1 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"index_gh/#mlperf-inference-v20-submission-02252022","title":"MLPerf Inference v2.0 (submission 02/25/2022)","text":"<p>Use the r2.0 branch (<code>git checkout r2.0</code>) if you want to submit or reproduce v2.0 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx imagenet2012 edge,datacenter ssd-mobilenet 300x300 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 edge ssd-resnet34 1200x1200 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"index_gh/#mlperf-inference-v11-submission-08132021","title":"MLPerf Inference v1.1 (submission 08/13/2021)","text":"<p>Use the r1.1 branch (<code>git checkout r1.1</code>) if you want to submit or reproduce v1.1 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx imagenet2012 edge,datacenter ssd-mobilenet 300x300 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 edge ssd-resnet34 1200x1200 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet pytorch, tensorflow(?), onnx(?) BraTS 2019 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"index_gh/#mlperf-inference-v10-submission-03192021","title":"MLPerf Inference v1.0 (submission 03/19/2021)","text":"<p>Use the r1.0 branch (<code>git checkout r1.0</code>) if you want to submit or reproduce v1.0 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx imagenet2012 edge,datacenter ssd-mobilenet 300x300 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 edge ssd-resnet34 1200x1200 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow(?) Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet pytorch, tensorflow(?), onnx(?) BraTS 2019 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"index_gh/#mlperf-inference-v07-submission-9182020","title":"MLPerf Inference v0.7 (submission 9/18/2020)","text":"<p>Use the r0.7 branch (<code>git checkout r0.7</code>) if you want to submit or reproduce v0.7 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset resnet50-v1.5 vision/classification_and_detection tensorflow, pytorch, onnx imagenet2012 ssd-mobilenet 300x300 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 ssd-resnet34 1200x1200 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 bert language/bert tensorflow, pytorch, onnx squad-1.1 dlrm recommendation/dlrm pytorch, tensorflow(?), onnx(?) Criteo Terabyte 3d-unet vision/medical_imaging/3d-unet pytorch, tensorflow(?), onnx(?) BraTS 2019 rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus"},{"location":"index_gh/#mlperf-inference-v05","title":"MLPerf Inference v0.5","text":"<p>Use the r0.5 branch (<code>git checkout r0.5</code>) if you want to reproduce v0.5 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset resnet50-v1.5 v0.5/classification_and_detection tensorflow, pytorch, onnx imagenet2012 mobilenet-v1 v0.5/classification_and_detection tensorflow, pytorch, onnx imagenet2012 ssd-mobilenet 300x300 v0.5/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 ssd-resnet34 1200x1200 v0.5/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 gnmt v0.5/translation/gnmt/ tensorflow, pytorch See Readme"},{"location":"benchmarks/automotive/3d_object_detection/get-pointpainting-data/","title":"3-D Object Detection using PointPainting","text":""},{"location":"benchmarks/automotive/3d_object_detection/get-pointpainting-data/#dataset","title":"Dataset","text":"<p>Note: By default, the waymo dataset is downloaded from the mlcommons official drive. One has to accept the MLCommons Waymo Open Dataset EULA to access the dataset files. </p> <p>The benchmark implementation run command will automatically download the preprocessed dataset. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_WAYMO_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/get-pointpainting-data/#get-validation-and-calibration-dataset","title":"Get Validation and Calibration Dataset","text":"<pre><code>mlcr get,dataset,waymo,_r2-downloader,_mlc -j\n</code></pre>"},{"location":"benchmarks/automotive/3d_object_detection/get-pointpainting-data/#get-calibration-dataset-only","title":"Get Calibration Dataset only","text":"<pre><code>mlcr get,dataset,waymo,calibration,_r2-downloader,_mlc -j\n</code></pre>"},{"location":"benchmarks/automotive/3d_object_detection/get-pointpainting-data/#model","title":"Model","text":"<p>Note: By default, the PointPainting is downloaded from the mlcommons official drive. One has to accept the MLCommons Waymo Open Dataset EULA to access the model files. </p> <p>The benchmark implementation run command will automatically download the model. In case you want to download only the PointPainting model, you can use the below command.</p> <pre><code>mlcr get,ml-model,pointpainting,_r2-downloader,_mlc -j\n</code></pre> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_POINTPAINTING_MODEL&gt;</code> could be provided to download the model files to a specific location.</li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/","title":"3D Object Detection using PointPainting","text":"MLCommons-Python"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>POINTPAINTING</p> edge"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#edge-category","title":"Edge category","text":"<p>In the edge category, pointpainting has SingleStream scenario and the scenario is  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDA"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 950GB</li> </ul> DockerNative"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt; --rerun\n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the SingleStream scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#singlestream","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt; \n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt; \n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt; --rerun\n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#singlestream_1","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt; \n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt; \n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 950GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt; --rerun\n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the SingleStream scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#singlestream_2","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt; \n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt; \n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt; --rerun\n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for pointpainting you can follow this README.</li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#singlestream_3","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt; \n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul>"},{"location":"benchmarks/automotive/3d_object_detection/pointpainting/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt; \n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul>"},{"location":"benchmarks/graph/get-rgat-data/","title":"Graph Neural Network using R-GAT","text":""},{"location":"benchmarks/graph/get-rgat-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration Full DatasetDebug Dataset <p>R-GAT validation run uses the IGBH dataset consisting of 547,306,935 nodes and 5,812,005,639 edges.</p> <p>R-GAT debug run uses the IGBH debug dataset(tiny).</p> <p>The calibration dataset contains 5000 nodes from the training paper nodes of the IGBH dataset. IGBH <code>full</code> dataset would be downloaded for creating calibration dataset. </p> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_IGBH_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/graph/get-rgat-data/#get-full-dataset","title":"Get Full Dataset","text":"<pre><code>mlcr get,dataset,igbh,_full -j\n</code></pre>"},{"location":"benchmarks/graph/get-rgat-data/#get-full-dataset_1","title":"Get Full Dataset","text":"<pre><code>mlcr get,dataset,igbh,_debug -j\n</code></pre>"},{"location":"benchmarks/graph/get-rgat-data/#get-calibration-dataset","title":"Get Calibration Dataset","text":"<pre><code>mlcr get,dataset,igbh,_full,_calibration -j\n</code></pre>"},{"location":"benchmarks/graph/get-rgat-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf R-GAT Model</p> PyTorch <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_RGAT_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/graph/get-rgat-data/#pytorch","title":"PyTorch","text":"<pre><code>mlcr get,ml-model,rgat,_r2-downloader,_mlcommons -j\n</code></pre>"},{"location":"benchmarks/graph/rgat/","title":"Graph Neural Network using R-GAT","text":"MLCommons-Python"},{"location":"benchmarks/graph/rgat/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>RGAT</p> datacenteredge"},{"location":"benchmarks/graph/rgat/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, rgat has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/graph/rgat/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDA"},{"location":"benchmarks/graph/rgat/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 2.3TB</li> </ul> DockerNative"},{"location":"benchmarks/graph/rgat/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/graph/rgat/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/graph/rgat/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#server","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/graph/rgat/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/graph/rgat/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/graph/rgat/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/graph/rgat/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/graph/rgat/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/graph/rgat/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#server_1","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/graph/rgat/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/graph/rgat/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/graph/rgat/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 2.3TB</p> </li> </ul> DockerNative"},{"location":"benchmarks/graph/rgat/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/graph/rgat/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/graph/rgat/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#server_2","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/graph/rgat/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/graph/rgat/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/graph/rgat/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/graph/rgat/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/graph/rgat/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/graph/rgat/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#server_3","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/graph/rgat/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/graph/rgat/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/graph/rgat/#edge-category","title":"Edge category","text":"<p>In the edge category, rgat has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/graph/rgat/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDA"},{"location":"benchmarks/graph/rgat/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 2.3TB</li> </ul> DockerNative"},{"location":"benchmarks/graph/rgat/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/graph/rgat/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/graph/rgat/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#singlestream","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/graph/rgat/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/graph/rgat/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/graph/rgat/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#singlestream_1","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 2.3TB</p> </li> </ul> DockerNative"},{"location":"benchmarks/graph/rgat/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/graph/rgat/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/graph/rgat/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_12","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#accuracy-only_12","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#singlestream_2","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_13","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#accuracy-only_13","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/graph/rgat/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/graph/rgat/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for rgat you can follow this README.</li> </ul>"},{"location":"benchmarks/graph/rgat/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_14","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#accuracy-only_14","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#singlestream_3","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/graph/rgat/#performance-only_15","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#accuracy-only_15","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/graph/rgat/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/get-resnet50-data/","title":"Image Classification using ResNet50","text":""},{"location":"benchmarks/image_classification/get-resnet50-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> UnprocessedPreprocessed ValidationCalibration <p>ResNet50 validation run uses the Imagenet 2012 validation dataset consisting of 50,000 images.</p> <p>ResNet50 calibration dataset consist of 500 images selected from the Imagenet 2012 validation dataset. There are 2 alternative options for the calibration dataset.</p> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_IMAGENET_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/image_classification/get-resnet50-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>mlcr get,dataset,imagenet,validation,_full -j\n</code></pre>"},{"location":"benchmarks/image_classification/get-resnet50-data/#get-calibration-dataset-using-option-1","title":"Get Calibration Dataset Using Option 1","text":"<pre><code>mlcr get,dataset,imagenet,calibration,_mlperf.option1 -j\n</code></pre>"},{"location":"benchmarks/image_classification/get-resnet50-data/#get-calibration-dataset-using-option-2","title":"Get Calibration Dataset Using Option 2","text":"<pre><code>mlcr get,dataset,imagenet,calibration,_mlperf.option2 -j\n</code></pre>"},{"location":"benchmarks/image_classification/get-resnet50-data/#get-resnet50-preprocessed-dataset","title":"Get ResNet50 preprocessed dataset","text":"<pre><code>mlcr get,dataset,image-classification,imagenet,preprocessed,_pytorch,_full-j\n</code></pre>"},{"location":"benchmarks/image_classification/get-resnet50-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf ResNet50 Model</p> TensorflowOnnx <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_RESNET50_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/image_classification/get-resnet50-data/#tensorflow","title":"Tensorflow","text":"<pre><code>mlcr get,ml-model,resnet50,_tensorflow -j\n</code></pre>"},{"location":"benchmarks/image_classification/get-resnet50-data/#onnx","title":"Onnx","text":"<pre><code>mlcr get,ml-model,resnet50,image-classification,_onnx -j\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/","title":"Image Classification using Mobilenet models","text":"<p>Install MLC following the installation page.</p> <p>Mobilenet models are not official MLPerf models and so cannot be used for a Closed division MLPerf inference submission. But since they can be run with Imagenet dataset, we are allowed to use them for Open division submission. Only CPU runs are supported now. </p>"},{"location":"benchmarks/image_classification/mobilenets/#tflite-backend","title":"TFLite Backend","text":"Mobilenet-V1Mobilenet-V2Mobilenet-V3MobilenetsEfficientnetMobilenets and Efficientnet"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v1","title":"Mobilenet V1","text":"<pre><code>mlcr run,mobilenet-models,_tflite,_mobilenet-v1 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v2","title":"Mobilenet V2","text":"<pre><code>mlcr run,mobilenet-models,_tflite,_mobilenet-v2 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v3","title":"Mobilenet V3","text":"<pre><code>mlcr run,mobilenet-models,_tflite,_mobilenet-v3 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v1v2v3","title":"Mobilenet V1,V2,V3","text":"<pre><code>mlcr run,mobilenet-models,_tflite,_mobilenet \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#efficientnet","title":"Efficientnet","text":"<pre><code>mlcr run,mobilenet-models,_tflite,_efficientnet \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenets-and-efficientnet","title":"Mobilenets and Efficientnet","text":"<pre><code>mlcr run,mobilenet-models,_tflite \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#armnn-backend","title":"ARMNN Backend","text":"Mobilenet-V1Mobilenet-V2Mobilenet-V3MobilenetsEfficientnetMobilenets and Efficientnet"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v1_1","title":"Mobilenet V1","text":"<pre><code>mlcr run,mobilenet-models,_tflite,_armnn,_mobilenet-v1 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v2_1","title":"Mobilenet V2","text":"<pre><code>mlcr run,mobilenet-models,_tflite,_armnn,_mobilenet-v2 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v2_2","title":"Mobilenet V2","text":"<pre><code>mlcr run,mobilenet-models,_tflite,_armnn,_mobilenet-v2 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v1v2v3_1","title":"Mobilenet V1,V2,V3","text":"<pre><code>mlcr run,mobilenet-models,_tflite,_armnn,_mobilenet \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#efficientnet_1","title":"Efficientnet","text":"<pre><code>mlcr run,mobilenet-models,_tflite,_armnn,_efficientnet \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenets-and-efficientnet_1","title":"Mobilenets and Efficientnet","text":"<pre><code>mlcr run,mobilenet-models,_tflite,_armnn \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/","title":"Image Classification using ResNet50","text":"MLCommons-PythonNvidia"},{"location":"benchmarks/image_classification/resnet50/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>RESNET50</p> datacenter"},{"location":"benchmarks/image_classification/resnet50/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> OnnxruntimeTensorflowDeepsparse"},{"location":"benchmarks/image_classification/resnet50/#onnxruntime-framework","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/image_classification/resnet50/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_1","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_2","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_3","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/image_classification/resnet50/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=1000 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_4","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#tensorflow-framework","title":"Tensorflow framework","text":"CPUCUDAROCm"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_5","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_12","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_12","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_6","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_13","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_13","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_14","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_14","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_7","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_15","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_15","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_8","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_16","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_16","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_8","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_17","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_17","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/image_classification/resnet50/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=1000 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_9","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_18","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_18","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_9","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_19","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_19","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#deepsparse-framework","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Docker"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub                === \"Native\"</p>"},{"location":"benchmarks/image_classification/resnet50/#offline_10","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_20","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_20","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_10","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_21","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_21","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_6","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub    === \"edge\"</p>"},{"location":"benchmarks/image_classification/resnet50/#offline_11","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_22","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_22","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_11","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_23","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_23","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#edge-category","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, MultiStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> OnnxruntimeTensorflowDeepsparse"},{"location":"benchmarks/image_classification/resnet50/#onnxruntime-framework_1","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_12","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_24","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_24","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_25","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_25","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_26","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_26","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_7","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_13","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_27","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_27","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_1","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_28","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_28","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_1","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_29","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_29","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_14","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_30","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_30","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_2","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_31","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_31","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_2","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_32","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_32","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_8","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_8","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_15","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_33","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_33","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_3","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_34","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_34","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_3","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_35","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_35","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/image_classification/resnet50/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_9","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_9","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=1000 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_16","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_36","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_36","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_4","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_37","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_37","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_4","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_38","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_38","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#tensorflow-framework_1","title":"Tensorflow framework","text":"CPUCUDAROCm"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_4","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_17","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_39","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_39","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_5","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_40","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_40","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_5","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_41","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_41","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_10","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_10","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_18","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_42","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_42","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_6","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_43","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_43","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_6","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_44","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_44","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_8","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_19","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_45","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_45","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_7","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_46","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_46","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_7","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_47","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_47","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_11","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_11","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_20","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_48","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_48","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_8","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_49","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_49","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_8","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_50","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_50","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/image_classification/resnet50/#native-environment_12","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_12","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_12","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=1000 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_21","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_51","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_51","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_9","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_52","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_52","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_9","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_53","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_53","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#deepsparse-framework_1","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_5","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Docker"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_9","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub                === \"Native\"</p>"},{"location":"benchmarks/image_classification/resnet50/#offline_22","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_54","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_54","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_10","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_55","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_55","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_10","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_56","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_56","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_13","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_13","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_13","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub     * If you want to download the official MLPerf model and dataset for resnet50 you can follow this README.</p> <ul> <li>Please see mobilenets.md for running mobilenet models for Image Classification.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_23","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_57","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_57","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_11","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_58","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_58","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_11","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_59","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_59","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>RESNET50</p> datacenteredge"},{"location":"benchmarks/image_classification/resnet50/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/image_classification/resnet50/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: To be updated</li> </ul> Docker"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_10","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.resnet50:8</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_24","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_60","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_60","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_12","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_61","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_61","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_24","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#edge-category_1","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, MultiStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/image_classification/resnet50/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: To be updated</li> </ul> Docker"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_11","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.resnet50:8</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_25","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_62","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_62","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_12","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_63","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_63","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_12","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/image_classification/resnet50/#performance-only_64","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#accuracy-only_64","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_25","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/","title":"Question Answering using Bert-Large","text":"MLCommons-PythonNvidia"},{"location":"benchmarks/language/bert/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>BERT-99</p> datacenteredge"},{"location":"benchmarks/language/bert/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, bert-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> PytorchDeepsparse"},{"location":"benchmarks/language/bert/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#server","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#server_1","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#server_2","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#server_3","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/bert/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#server_4","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#deepsparse-framework","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/language/bert/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#server_5","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_12","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_12","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#server_6","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_13","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_13","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#edge-category","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> PytorchDeepsparse"},{"location":"benchmarks/language/bert/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_14","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_14","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_15","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_15","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_8","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_16","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_16","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_1","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_17","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_17","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_9","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_18","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_18","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_2","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_19","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_19","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_10","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_20","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_20","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_3","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_21","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_21","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/bert/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_6","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_11","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_22","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_22","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_4","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_23","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_23","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#deepsparse-framework_1","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/language/bert/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_12","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_24","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_24","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_5","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_25","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_25","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_7","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul> <p>BERT-99.9</p> datacenter"},{"location":"benchmarks/language/bert/#offline_13","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_26","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_26","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_6","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_27","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_27","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> PytorchDeepsparse"},{"location":"benchmarks/language/bert/#pytorch-framework_2","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device_4","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_14","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_28","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_28","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#server_7","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_29","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_29","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_9","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_15","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_30","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_30","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#server_8","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_31","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_31","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_16","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_32","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_32","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#server_9","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_33","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_33","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_17","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_34","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_34","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#server_10","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_35","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_35","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/bert/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_12","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/bert/#offline_18","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_36","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_36","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#server_11","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_37","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_37","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#deepsparse-framework_2","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/language/bert/#cpu-device_5","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_13","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_19","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_38","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_38","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#server_12","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_39","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_39","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_14","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for bert-99.9 you can follow this README.</li> </ul>"},{"location":"benchmarks/language/bert/#offline_20","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_40","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_40","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/bert/#server_13","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_41","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_41","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>BERT-99</p> datacenteredge"},{"location":"benchmarks/language/bert/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, bert-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/bert/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/bert/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: To be updated</li> </ul> Docker"},{"location":"benchmarks/language/bert/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/language/bert/#offline_21","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_42","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_42","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#server_14","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_43","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_43","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#edge-category_1","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/bert/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/bert/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: To be updated</li> </ul> Docker"},{"location":"benchmarks/language/bert/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>BERT-99.9</p> datacenter"},{"location":"benchmarks/language/bert/#offline_22","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_44","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_44","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_7","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_45","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_45","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/bert/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/bert/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: To be updated</li> </ul> Docker"},{"location":"benchmarks/language/bert/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_15","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>s</p>"},{"location":"benchmarks/language/bert/#offline_23","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_46","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#accuracy-only_46","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/bert/#server_15","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/bert/#performance-only_47","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#accuracy-only_47","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/","title":"Reasoning using DeepSeek-R1","text":"MLCommons-Python"},{"location":"benchmarks/language/deepseek-r1/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>DEEPSEEK-R1</p> datacenter"},{"location":"benchmarks/language/deepseek-r1/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, deepseek-r1 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> vLLMPytorchSGLang"},{"location":"benchmarks/language/deepseek-r1/#vllm-framework","title":"vLLM framework","text":"CUDA"},{"location":"benchmarks/language/deepseek-r1/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: To be updated</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/deepseek-r1/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/deepseek-r1/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_USE_ML_MODEL_FROM_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_USE_DATASET_FROM_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/deepseek-r1/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#server","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/deepseek-r1/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/deepseek-r1/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#server_1","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/deepseek-r1/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#pytorch-framework","title":"Pytorch framework","text":"CUDA"},{"location":"benchmarks/language/deepseek-r1/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: To be updated</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/deepseek-r1/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/deepseek-r1/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_USE_ML_MODEL_FROM_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_USE_DATASET_FROM_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/deepseek-r1/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#server_2","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/deepseek-r1/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/deepseek-r1/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#server_3","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/deepseek-r1/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#sglang-framework","title":"SGLang framework","text":"CUDA"},{"location":"benchmarks/language/deepseek-r1/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: To be updated</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/deepseek-r1/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/deepseek-r1/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_USE_ML_MODEL_FROM_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_USE_DATASET_FROM_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=sglang \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/deepseek-r1/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=sglang \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=sglang \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#server_4","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/deepseek-r1/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=sglang \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=sglang \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=sglang \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=sglang \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for deepseek-r1 you can follow this README.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/deepseek-r1/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=sglang \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=sglang \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/deepseek-r1/#server_5","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/deepseek-r1/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=sglang \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=sglang \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/deepseek-r1/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=deepseek-r1 \\\n   --implementation=reference \\\n   --framework=sglang \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/get-bert-data/","title":"Question Answering using Bert-Large","text":""},{"location":"benchmarks/language/get-bert-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration <p>BERT validation run uses the SQuAD v1.1 dataset.</p> Calibration Set 1Calibration Set 2 <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_SQUAD_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/language/get-bert-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>mlcr get,dataset,squad,validation -j\n</code></pre>"},{"location":"benchmarks/language/get-bert-data/#get-calibration-dataset","title":"Get Calibration Dataset","text":"<pre><code>mlcr get,dataset,squad,_calib1 -j\n</code></pre>"},{"location":"benchmarks/language/get-bert-data/#get-calibration-dataset_1","title":"Get Calibration Dataset","text":"<pre><code>mlcr get,dataset,squad,_calib2 -j\n</code></pre>"},{"location":"benchmarks/language/get-bert-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf Bert-Large Model</p> PytorchOnnxTensorflow <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_BERT_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/language/get-bert-data/#pytorch","title":"Pytorch","text":"<pre><code>mlcr get,ml-model,bert-large,_pytorch -j\n</code></pre>"},{"location":"benchmarks/language/get-bert-data/#onnx","title":"Onnx","text":"<pre><code>mlcr get,ml-model,bert-large,_onnx -j\n</code></pre>"},{"location":"benchmarks/language/get-bert-data/#tensorflow","title":"Tensorflow","text":"<pre><code>mlcr get,ml-model,bert-large,_tensorflow -j\n</code></pre>"},{"location":"benchmarks/language/get-deepseek-r1-data/","title":"Reasoning using DeepSeek R1","text":""},{"location":"benchmarks/language/get-deepseek-r1-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>### Get Validation Dataset   <pre><code>mlcr get,preprocessed,dataset,deepseek-r1,_validation,_mlc,_r2-downloader --outdirname=&lt;path to download&gt; -j\n</code></pre></p> <p>### Get Calibration Dataset   <pre><code>  mlcr get,preprocessed,dataset,deepseek-r1,_calibration,_mlc,_r2-downloader --outdirname=&lt;path to download&gt; -j\n</code></pre></p>"},{"location":"benchmarks/language/get-deepseek-r1-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> Pytorch From MLCOMMONS Storage"},{"location":"benchmarks/language/get-deepseek-r1-data/#get-the-official-mlperf-deekseek-r1-model-from-mlcommons-storage","title":"Get the Official MLPerf DeekSeek-R1 model from MLCOMMONS Storage","text":"<pre><code>mlcr get,ml-model,deepseek-r1,_r2-downloader,_mlc,_dry-run -j\n</code></pre>"},{"location":"benchmarks/language/get-gptj-data/","title":"Text Summarization using GPT-J","text":""},{"location":"benchmarks/language/get-gptj-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration <p>GPT-J validation run uses the CNNDM dataset.</p> <p>GPT-J calibration dataset is extracted from the CNNDM dataset.</p> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_CNNDM_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/language/get-gptj-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>mlcr get,dataset,cnndm,_validation -j\n</code></pre>"},{"location":"benchmarks/language/get-gptj-data/#get-validation-dataset_1","title":"Get Validation Dataset","text":"<pre><code>mlcr get,dataset,cnndm,_calibration -j\n</code></pre>"},{"location":"benchmarks/language/get-gptj-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf GPT-J Model</p> Pytorch <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_GPTJ_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/language/get-gptj-data/#pytorch","title":"Pytorch","text":"<pre><code>mlcr get,ml-model,gptj,_fp32,_pytorch,_r2-downloader -j\n</code></pre>"},{"location":"benchmarks/language/get-llama2-70b-data/","title":"Text Summarization using LLAMA2-70b","text":""},{"location":"benchmarks/language/get-llama2-70b-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Nvidia Preprocessed DatasetMLCommons Preprocessed DatasetUnprocessed Dataset ValidationCalibration <p>LLAMA2-70b validation run uses the Open ORCA dataset.</p> ValidationCalibration <p>LLAMA2-70b validation run uses the Open ORCA dataset.</p> ValidationCalibration <p>LLAMA2-70b validation run uses the Open ORCA dataset.</p> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_OPENORCA_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/language/get-llama2-70b-data/#get-preprocessed-validation-dataset","title":"Get Preprocessed Validation Dataset","text":"<pre><code>mlcr get,dataset,preprocessed,openorca,_validation,_mlcommons,_nvidia -j\n</code></pre>"},{"location":"benchmarks/language/get-llama2-70b-data/#get-preprocessed-calibration-dataset","title":"Get Preprocessed Calibration dataset","text":"<pre><code>mlcr get,dataset,preprocessed,openorca,_calibration,_mlcommons,_nvidia -j\n</code></pre>"},{"location":"benchmarks/language/get-llama2-70b-data/#get-preprocessed-validation-dataset_1","title":"Get Preprocessed Validation Dataset","text":"<pre><code>mlcr get,dataset,preprocessed,openorca,_validation,_r2-downloader,_mlc -j\n</code></pre>"},{"location":"benchmarks/language/get-llama2-70b-data/#get-preprocessed-calibration-dataset_1","title":"Get Preprocessed Calibration dataset","text":"<p><pre><code>mlcr get,dataset,preprocessed,openorca,_calibration,_r2-downloader,_mlc -j\n</code></pre> ```</p>"},{"location":"benchmarks/language/get-llama2-70b-data/#get-unprocessed-validation-dataset","title":"Get Unprocessed Validation Dataset","text":"<pre><code>mlcr get,dataset,openorca,_validation -j\n</code></pre>"},{"location":"benchmarks/language/get-llama2-70b-data/#get-unprocessed-validation-dataset_1","title":"Get Unprocessed Validation Dataset","text":"<pre><code>mlcr get,dataset,openorca,_validation -j\n</code></pre>"},{"location":"benchmarks/language/get-llama2-70b-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> Full-Precision ModelsQuantized Models Pytorch From MLCOMMONS Storage <p>Note:  One has to accept the MLCommons Llama 2 License Confidentiality Notice to access the model files in MLCOMMONS Storage. </p> Using TensorRT-LLM Round v5.1Round v5.0 Quantize Locally <p>Note:  One has to accept the MLCommons Llama 2 License Confidentiality Notice to access the full precision model files in MLCOMMONS storage which are needed for quantization process.</p> <pre><code>mlcr get,ml-model,llama2-70b,_nvidia,_fp8,_v5.1 -j\n</code></pre> <ul> <li>Use <code>--checkpoint=&lt;Full Precision model path&gt;</code> if model is already downloaded to a specific location.</li> </ul> Quantize LocallyPre-Quantized Model from MLCOMMONS Storage <p>Note:  One has to accept the MLCommons Llama 2 License Confidentiality Notice to access the full precision model files in MLCOMMONS storage which are needed for quantization process.</p> <pre><code>mlcr get,ml-model,llama2-70b,_nvidia,_fp8,_v5.0 -j\n</code></pre> <ul> <li>Use <code>--checkpoint=&lt;Full Precision model path&gt;</code> if model is already downloaded to a specific location.</li> </ul> <p>Note:  One has to accept the MLCommons Llama 2 License Confidentiality Notice to access the full precision model files and pre-quantized model files in MLCOMMONS storage.</p> <pre><code>mlcr get,ml-model,llama2-70b,_nvidia,_fp8,_v5.0,_pre-quantized -j\n</code></pre> <ul> <li>Use <code>--checkpoint=&lt;Full Precision model path&gt;</code> if full precision model is already downloaded to a specific location.</li> </ul> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_LLAMA2_70B_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/language/get-llama2-70b-data/#get-the-official-mlperf-llama2-70b-model-from-mlcommons-storage","title":"Get the Official MLPerf LLAMA2-70B model from MLCOMMONS Storage","text":"<pre><code>mlcr get,ml-model,llama2-70b,_pytorch,_r2-downloader,_70b,_mlc -j\n</code></pre>"},{"location":"benchmarks/language/get-llama2-70b-data/#get-the-official-mlperf-llama2-70b-model-from-mlcommons-google-drive","title":"Get the Official MLPerf LLAMA2-70B model from MLCOMMONS Google Drive","text":"<pre><code>=== \"From MLCOMMONS Cloudfare R2\"\n\n&gt; **Note:**  One has to accept the [MLCommons Llama 2 License Confidentiality Notice](https://llama2.mlcommons.org/) to access the model files in MLCOMMONS Google Drive.\n\n### Get the Official MLPerf LLAMA2-70B model from MLCOMMONS Cloudfare R2\n\n```\nmlcr get,ml-model,llama2-70b,_mlc,_r2-downloader,_70b -j\n```\n=== \"From Hugging Face repo\"\n\n    &gt; **Note:** Access to the HuggingFace model could be requested [here](https://ai.meta.com/resources/models-and-libraries/llama-downloads/).\n\n    ### Get model from HuggingFace repo\n    ```\n    mlcr get,ml-model,llama2-70b,_hf --hf_token=&lt;huggingface access token&gt; -j\n    ```\n</code></pre>"},{"location":"benchmarks/language/get-llama3_1-405b-data/","title":"Text Summarization using LLAMA3.1-405b","text":""},{"location":"benchmarks/language/get-llama3_1-405b-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_LLAMA3_405B_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/language/get-llama3_1-405b-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>mlcr get,dataset,mlperf,inference,llama3,_validation,_r2-downloader --outdirname=&lt;path to download&gt; -j\n</code></pre>"},{"location":"benchmarks/language/get-llama3_1-405b-data/#get-calibration-dataset","title":"Get Calibration Dataset","text":"<pre><code>mlcr get,dataset,mlperf,inference,llama3,_calibration,_r2-downloader --outdirname=&lt;path to download&gt; -j\n</code></pre>"},{"location":"benchmarks/language/get-llama3_1-405b-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> Pytorch From MLCOMMONS StorageFrom Cloudfare R2From Hugging Face repo <p>Note:  One has to accept the MLCommons Llama 3.1 License Confidentiality Notice to access the model files in MLCOMMONS Storage. </p> <p>Note:  One has to accept the MLCommons Llama 3.1 License Confidentiality Notice to access the model files in MLCOMMONS Google Drive. </p>"},{"location":"benchmarks/language/get-llama3_1-405b-data/#get-the-official-mlperf-llama31-405b-model-from-mlcommons-google-drive","title":"Get the Official MLPerf LLAMA3.1-405B model from MLCOMMONS Google Drive","text":"<pre><code>mlcr get,ml-model,llama3,_mlc,_r2-downloader,_405b --outdirname=&lt;path to download&gt; -j\n</code></pre>"},{"location":"benchmarks/language/get-llama3_1-405b-data/#get-the-official-mlperf-llama31-405b-model-from-mlcommons-cloudfare-r2","title":"Get the Official MLPerf LLAMA3.1-405B model from MLCOMMONS Cloudfare R2","text":"<p>``` mlcr get,ml-model,llama3,_mlc,_405b,_r2-downloader    --outdirname= -j <p>Note: Access to the HuggingFace model could be requested here.</p> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_LLAMA3_405B_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/language/get-llama3_1-405b-data/#get-model-from-huggingface-repo","title":"Get model from HuggingFace repo","text":"<pre><code>mlcr get,ml-model,llama3,_hf --hf_token=&lt;huggingface access token&gt; -j\n</code></pre>"},{"location":"benchmarks/language/get-llama3_1-8b-data/","title":"Text Summarization using LLAMA3.1-8b","text":""},{"location":"benchmarks/language/get-llama3_1-8b-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration Full dataset (Datacenter)5000 samples (Edge) <pre><code>```\n</code></pre> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_LLAMA3_405B_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/language/get-llama3_1-8b-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>mlcr get,dataset,cnndm,_validation,_datacenter,_llama3,_mlc,_r2-downloader --outdirname=&lt;path to download&gt; -j\n</code></pre>"},{"location":"benchmarks/language/get-llama3_1-8b-data/#get-validation-dataset_1","title":"Get Validation Dataset","text":"<pre><code>mlcr get,dataset,cnndm,_validation,_edge,_llama3,_mlc,_r2-downloader --outdirname=&lt;path to download&gt; -j\n</code></pre>"},{"location":"benchmarks/language/get-llama3_1-8b-data/#get-calibration-dataset","title":"Get Calibration Dataset","text":"<pre><code>mlcr get,dataset,cnndm,_calibration,_llama3,_mlc,_r2-downloader --outdirname=&lt;path to download&gt; -j\n</code></pre>"},{"location":"benchmarks/language/get-llama3_1-8b-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> Pytorch From Cloudfare R2From Hugging Face repo <p>Note:  One has to accept the MLCommons Llama 3.1 License Confidentiality Notice to access the model files in MLCOMMONS Storage. </p> <p>Note: Access to the HuggingFace model could be requested here.</p> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_LLAMA3_8B_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/language/get-llama3_1-8b-data/#get-the-official-mlperf-llama31-8b-model-from-mlcommons-cloudfare-r2","title":"Get the Official MLPerf LLAMA3.1-8B model from MLCOMMONS Cloudfare R2","text":"<pre><code>mlcr get,ml-model,llama3,_mlc,_8b,_r2-downloader --outdirname=&lt;path to download&gt; -j\n</code></pre>"},{"location":"benchmarks/language/get-llama3_1-8b-data/#get-model-from-huggingface-repo","title":"Get model from HuggingFace repo","text":"<pre><code>mlcr get,ml-model,llama3,_hf,_meta-llama/Llama-3.1-8B-Instruct --hf_token=&lt;huggingface access token&gt; -j\n</code></pre>"},{"location":"benchmarks/language/get-mixtral-8x7b-data/","title":"Get mixtral 8x7b data","text":""},{"location":"benchmarks/language/get-mixtral-8x7b-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the preprocessed validation and calibration datasets. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration <p>mixtral-8x7b validation run uses the combined dataset - Open ORCA, GSM8K and MBXP.</p> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_MIXTRAL_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/language/get-mixtral-8x7b-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>mlcr get,dataset-mixtral,openorca-mbxp-gsm8k-combined,_r2-downloader,_validation -j\n</code></pre>"},{"location":"benchmarks/language/get-mixtral-8x7b-data/#get-calibration-dataset","title":"Get Calibration Dataset","text":"<pre><code>mlcr get,dataset-mixtral,openorca-mbxp-gsm8k-combined,_r2-downloader,_calibration -j\n</code></pre>"},{"location":"benchmarks/language/get-mixtral-8x7b-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf MIXTRAL-8x7b Model</p> Pytorch <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_MIXTRAL_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/language/get-mixtral-8x7b-data/#pytorch","title":"Pytorch","text":"<pre><code>mlcr get,ml-model,mixtral,_r2-downloader,_mlc -j\n</code></pre>"},{"location":"benchmarks/language/gpt-j/","title":"Text Summarization using GPT-J","text":"MLCommons-PythonNvidia"},{"location":"benchmarks/language/gpt-j/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>GPTJ-99</p> datacenteredge"},{"location":"benchmarks/language/gpt-j/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/gpt-j/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_1","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB(fp32). 40GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_2","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_3","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/gpt-j/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_4","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#edge-category","title":"Edge category","text":"<p>In the edge category, gptj-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/gpt-j/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_12","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_12","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_1","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_13","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_13","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB(fp32). 40GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_14","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_14","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_2","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_15","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_15","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_8","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_16","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_16","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_3","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_17","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_17","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/gpt-j/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>GPTJ-99.9</p> datacenteredge"},{"location":"benchmarks/language/gpt-j/#offline_9","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_18","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_18","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_4","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_19","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_19","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework_2","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/gpt-j/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_6","title":"Performance Estimation for Offline Scenario","text":"<pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_10","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_20","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_20","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_5","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_21","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_21","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_7","title":"Performance Estimation for Offline Scenario","text":"<pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_11","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_22","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_22","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_6","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_23","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_23","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB(fp32). 40GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_12","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_24","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_24","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_7","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_25","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_25","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_9","title":"Performance Estimation for Offline Scenario","text":"<pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_13","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_26","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_26","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_8","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_27","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_27","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/gpt-j/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_14","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_28","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_28","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_9","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_29","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_29","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#edge-category_1","title":"Edge category","text":"<p>In the edge category, gptj-99.9 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework_3","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/gpt-j/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_15","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_30","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_30","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_5","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_31","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_31","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_12","title":"Performance Estimation for Offline Scenario","text":"<pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_16","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_32","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_32","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_6","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_33","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_33","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB(fp32). 40GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_13","title":"Performance Estimation for Offline Scenario","text":"<pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_17","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_34","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_34","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_7","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_35","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_35","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_14","title":"Performance Estimation for Offline Scenario","text":"<pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_18","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_36","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_36","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_8","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_37","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_37","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/gpt-j/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_15","title":"Performance Estimation for Offline Scenario","text":"<pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for gptj-99.9 you can follow this README.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_19","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_38","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_38","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_9","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_39","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_39","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>GPTJ-99</p> datacenteredge"},{"location":"benchmarks/language/gpt-j/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/gpt-j/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/gpt-j/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: 16GB</li> </ul> Docker"},{"location":"benchmarks/language/gpt-j/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.gptj:192</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_20","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_40","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_40","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#server_10","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_41","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_41","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#edge-category_2","title":"Edge category","text":"<p>In the edge category, gptj-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/gpt-j/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/gpt-j/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: 16GB</li> </ul> Docker"},{"location":"benchmarks/language/gpt-j/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.gptj:192</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>GPTJ-99.9</p> datacenteredge"},{"location":"benchmarks/language/gpt-j/#offline_21","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_42","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_42","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#singlestream_10","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_43","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_43","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/gpt-j/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/gpt-j/#cuda-device_6","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: 16GB</li> </ul> Docker"},{"location":"benchmarks/language/gpt-j/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_16","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_22","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_44","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_44","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#server_11","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_45","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_45","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#edge-category_3","title":"Edge category","text":"<p>In the edge category, gptj-99.9 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/gpt-j/#tensorrt-framework_3","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/gpt-j/#cuda-device_7","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: 16GB</li> </ul> Docker"},{"location":"benchmarks/language/gpt-j/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_17","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_23","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_46","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_46","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#singlestream_11","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/gpt-j/#performance-only_47","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#accuracy-only_47","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/","title":"Text Summarization using LLAMA2-70b","text":"MLCommons-PythonNvidia"},{"location":"benchmarks/language/llama2-70b/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>LLAMA2-70B-99</p> datacenteredge"},{"location":"benchmarks/language/llama2-70b/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/llama2-70b/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/llama2-70b/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_1","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_2","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_3","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</li> </ul> Native"},{"location":"benchmarks/language/llama2-70b/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_4","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#edge-category","title":"Edge category","text":"<p>In the edge category, llama2-70b-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/llama2-70b/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/llama2-70b/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#singlestream","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_12","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_12","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#singlestream_1","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_13","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_13","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_14","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_14","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#singlestream_2","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_15","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_15","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_8","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_16","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_16","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#singlestream_3","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_17","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_17","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</li> </ul> Native"},{"location":"benchmarks/language/llama2-70b/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>LLAMA2-70B-99.9</p> datacenteredge"},{"location":"benchmarks/language/llama2-70b/#offline_9","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_18","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_18","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#singlestream_4","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_19","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_19","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/llama2-70b/#pytorch-framework_2","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/llama2-70b/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_6","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_10","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_20","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_20","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_5","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_21","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_21","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_7","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_11","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_22","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_22","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_6","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_23","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_23","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_12","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_24","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_24","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_7","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_25","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_25","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_9","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_13","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_26","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_26","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_8","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_27","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_27","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</li> </ul> Native"},{"location":"benchmarks/language/llama2-70b/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_14","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_28","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_28","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_9","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_29","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_29","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#edge-category_1","title":"Edge category","text":"<p>In the edge category, llama2-70b-99.9 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/llama2-70b/#pytorch-framework_3","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/llama2-70b/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_15","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_30","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_30","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#singlestream_5","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_31","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_31","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_12","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_16","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_32","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_32","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#singlestream_6","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_33","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_33","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_13","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_17","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_34","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_34","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#singlestream_7","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_35","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_35","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_14","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_18","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_36","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_36","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#singlestream_8","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_37","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_37","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</li> </ul> Native"},{"location":"benchmarks/language/llama2-70b/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_15","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for llama2-70b-99.9 you can follow this README.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_19","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_38","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_38","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#singlestream_9","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_39","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_39","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>LLAMA2-70B-99</p> datacenteredge"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/llama2-70b/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/llama2-70b/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 2x80GB</p> </li> <li> <p>Disk Space:  900GB for manual execution of vendor implementation and 1.5TB for automated run through MLC-Scripts</p> </li> </ul> Docker"},{"location":"benchmarks/language/llama2-70b/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.llama2-70b:1024</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 \\\n   --tp_size=2 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used.</li> <li>Add <code>--adr.llama2-model.tags=_pre-quantized</code> to use the Nvidia quantized models with the available in the MLC Storage. These models were quantized with three different configurations of tensor parallelism and pipeline parallelism: TP1\u2013PP2, TP2\u2013PP1, and TP1\u2013PP1. The appropriate model will be automatically selected based on the values provided for <code>--tp_size</code> and <code>--pp_size</code> in run command. By default tp size of 2 and pp size of 1 would be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_20","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_40","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_40","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_10","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_41","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_41","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#edge-category_2","title":"Edge category","text":"<p>In the edge category, llama2-70b-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/llama2-70b/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/llama2-70b/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 2x80GB</p> </li> <li> <p>Disk Space:  900GB for manual execution of vendor implementation and 1.5TB for automated run through MLC-Scripts</p> </li> </ul> Docker"},{"location":"benchmarks/language/llama2-70b/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.llama2-70b:1024</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 \\\n   --tp_size=2 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used.</li> <li>Add <code>--adr.llama2-model.tags=_pre-quantized</code> to use the Nvidia quantized models with the available in the MLC Storage. These models were quantized with three different configurations of tensor parallelism and pipeline parallelism: TP1\u2013PP2, TP2\u2013PP1, and TP1\u2013PP1. The appropriate model will be automatically selected based on the values provided for <code>--tp_size</code> and <code>--pp_size</code> in run command. By default tp size of 2 and pp size of 1 would be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>LLAMA2-70B-99.9</p> datacenteredge"},{"location":"benchmarks/language/llama2-70b/#offline_21","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_42","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_42","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#singlestream_10","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_43","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_43","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/llama2-70b/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/llama2-70b/#cuda-device_6","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 2x80GB</p> </li> <li> <p>Disk Space:  900GB for manual execution of vendor implementation and 1.5TB for automated run through MLC-Scripts</p> </li> </ul> Docker"},{"location":"benchmarks/language/llama2-70b/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_16","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --tp_size=2 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_22","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_44","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_44","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_11","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_45","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_45","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#edge-category_3","title":"Edge category","text":"<p>In the edge category, llama2-70b-99.9 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/llama2-70b/#tensorrt-framework_3","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/llama2-70b/#cuda-device_7","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 2x80GB</p> </li> <li> <p>Disk Space:  900GB for manual execution of vendor implementation and 1.5TB for automated run through MLC-Scripts</p> </li> </ul> Docker"},{"location":"benchmarks/language/llama2-70b/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_17","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --tp_size=2 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_23","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_46","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_46","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#singlestream_11","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama2-70b/#performance-only_47","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#accuracy-only_47","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/","title":"Text Summarization using LLAMA3_1-405b","text":"MLCommons-Python"},{"location":"benchmarks/language/llama3_1-405b/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>LLAMA3_1-405B-99</p> datacenter"},{"location":"benchmarks/language/llama3_1-405b/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, llama3_1-405b-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/llama3_1-405b/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDA"},{"location":"benchmarks/language/llama3_1-405b/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 2.3TB</li> </ul> DockerNative"},{"location":"benchmarks/language/llama3_1-405b/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama3_1-405b/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_MLPERF_MODEL_LLAMA3_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_MLPERF_DATASET_LLAMA3_DOWNLOAD_TO_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#server","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama3_1-405b/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#server_1","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 2.3TB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/llama3_1-405b/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama3_1-405b/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_MLPERF_MODEL_LLAMA3_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_MLPERF_DATASET_LLAMA3_DOWNLOAD_TO_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#server_2","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>LLAMA3_1-405B-99.9</p> datacenter"},{"location":"benchmarks/language/llama3_1-405b/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#server_3","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, llama3_1-405b-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/llama3_1-405b/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDA"},{"location":"benchmarks/language/llama3_1-405b/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 2.3TB</li> </ul> DockerNative"},{"location":"benchmarks/language/llama3_1-405b/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama3_1-405b-99.</p>"},{"location":"benchmarks/language/llama3_1-405b/#performance-estimation-for-offline-scenario_2","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#server_4","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama3_1-405b-99.</p>"},{"location":"benchmarks/language/llama3_1-405b/#performance-estimation-for-offline-scenario_3","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#server_5","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 2.3TB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/llama3_1-405b/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama3_1-405b-99.</p>"},{"location":"benchmarks/language/llama3_1-405b/#performance-estimation-for-offline-scenario_4","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_12","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_12","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#server_6","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_13","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_13","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for llama3_1-405b-99.</p>"},{"location":"benchmarks/language/llama3_1-405b/#performance-estimation-for-offline-scenario_5","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for llama3_1-405b-99.9 you can follow this README.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_14","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_14","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-405b/#server_7","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-405b/#performance-only_15","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#accuracy-only_15","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-405b/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/","title":"Text Summarization using LLAMA3_1-8b","text":"MLCommons-Python"},{"location":"benchmarks/language/llama3_1-8b/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>LLAMA3_1-8B</p> datacenteredge"},{"location":"benchmarks/language/llama3_1-8b/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, llama3_1-8b has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> vLLM"},{"location":"benchmarks/language/llama3_1-8b/#vllm-framework","title":"vLLM framework","text":"CPUCUDA"},{"location":"benchmarks/language/llama3_1-8b/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: To be updated</li> </ul> DockerNative"},{"location":"benchmarks/language/llama3_1-8b/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama3_1-8b/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_USE_ML_MODEL_FROM_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_USE_DATASET_FROM_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#server","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama3_1-8b/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#server_1","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: To be updated</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/llama3_1-8b/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama3_1-8b/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_USE_ML_MODEL_FROM_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_USE_DATASET_FROM_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#server_2","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#server_3","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-8b \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#edge-category","title":"Edge category","text":"<p>In the edge category, llama3_1-8b-edge has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> vLLM"},{"location":"benchmarks/language/llama3_1-8b/#vllm-framework_1","title":"vLLM framework","text":"CPUCUDA"},{"location":"benchmarks/language/llama3_1-8b/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: To be updated</li> </ul> DockerNative"},{"location":"benchmarks/language/llama3_1-8b/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama3_1-8b/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_USE_ML_MODEL_FROM_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_USE_DATASET_FROM_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#singlestream","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama3_1-8b/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#singlestream_1","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: To be updated</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/llama3_1-8b/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama3_1-8b/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_USE_ML_MODEL_FROM_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_USE_DATASET_FROM_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_12","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_12","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#singlestream_2","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_13","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_13","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for llama3_1-8b-edge you can follow this README.</li> </ul>"},{"location":"benchmarks/language/llama3_1-8b/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_14","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_14","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#singlestream_3","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/llama3_1-8b/#performance-only_15","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#accuracy-only_15","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/llama3_1-8b/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=llama3_1-8b-edge \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/","title":"Question Answering, Math, and Code Generation using Mixtral-8x7B","text":"MLCommons-Python"},{"location":"benchmarks/language/mixtral-8x7b/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>MIXTRAL-8X7B</p> datacenteredge"},{"location":"benchmarks/language/mixtral-8x7b/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, mixtral-8x7b has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/mixtral-8x7b/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/mixtral-8x7b/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 100GB</li> </ul> DockerNative"},{"location":"benchmarks/language/mixtral-8x7b/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/mixtral-8x7b/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#server","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/mixtral-8x7b/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#server_1","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 4x80GB</p> </li> <li> <p>Disk Space: 100GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/mixtral-8x7b/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/mixtral-8x7b/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#server_2","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#server_3","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 100GB</li> </ul> Native"},{"location":"benchmarks/language/mixtral-8x7b/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/mixtral-8x7b/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#server_4","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#edge-category","title":"Edge category","text":"<p>In the edge category, mixtral-8x7b has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/mixtral-8x7b/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/mixtral-8x7b/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 100GB</li> </ul> DockerNative"},{"location":"benchmarks/language/mixtral-8x7b/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/mixtral-8x7b/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#singlestream","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/mixtral-8x7b/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_12","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_12","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#singlestream_1","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_13","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_13","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 4x80GB</p> </li> <li> <p>Disk Space: 100GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/mixtral-8x7b/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/mixtral-8x7b/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_14","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_14","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#singlestream_2","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_15","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_15","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline_8","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_16","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_16","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#singlestream_3","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_17","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_17","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 100GB</li> </ul> Native"},{"location":"benchmarks/language/mixtral-8x7b/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/mixtral-8x7b/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for mixtral-8x7b you can follow this README.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline_9","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_18","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_18","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#singlestream_4","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/mixtral-8x7b/#performance-only_19","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#accuracy-only_19","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/","title":"Question and Answering using Bert Large for IndySCC 2024","text":""},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#introduction","title":"Introduction","text":"<p>This guide is designed for the IndySCC 2024 to walk participants through running and optimizing the MLPerf Inference Benchmark using Bert Large across various software and hardware configurations. The goal is to maximize system throughput (measured in samples per second) without compromising accuracy.</p> <p>For a valid MLPerf inference submission, two types of runs are required: a performance run and an accuracy run. In this competition, we focus on the <code>Offline</code> scenario, where throughput is the key metric\u2014higher values are better. The official MLPerf inference benchmark for Bert Large requires processing a minimum of 10833 samples in both performance and accuracy modes using the Squad v1.1 dataset.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#scoring","title":"Scoring","text":"<p>In the IndySCC 2024, your objective will be to run a reference (unoptimized) Python implementation of the MLPerf inference benchmark to complete a successful submission passing the submission checker. Only one of the available framework needs to be submitted.</p> <p>Info</p> <p>Both MLPerf and MLC automation are evolving projects. If you encounter issues or have questions, please submit them here</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#artifacts-to-submit-to-the-scc-committee","title":"Artifacts to submit to the SCC committee","text":"<p>All the needed files are automatically pushed to the GitHub repository if you manage to complete the given commands. No additional files need to be submitted.</p> MLCommons-Python"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>BERT-99</p> edge"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#edge-category","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline scenario and the scenario is  mandatory for a closed division submission.</p> PytorchDeepsparse"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#deepsparse-framework","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#submission-commands","title":"Submission Commands","text":""},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#generate-actual-submission-tree","title":"Generate actual submission tree","text":"<pre><code>mlcr generate,inference,submission \\\n   --clean \\\n   --run-checker \\\n   --tar=yes \\\n   --env.CM_TAR_OUTFILE=submission.tar.gz \\\n   --division=open \\\n   --category=edge \\\n   --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n   --run_style=test \\\n   --quiet \\\n   --submitter=&lt;Team Name&gt;\n</code></pre> <ul> <li>Use <code>--hw_name=\"My system name\"</code> to give a meaningful system name.</li> </ul>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#push-results-to-github","title":"Push Results to GitHub","text":"<p>Fork the <code>mlperf-inference-results-scc24</code> branch of the repository URL at https://github.com/mlcommons/cm4mlperf-inference.</p> <p>Run the following command after replacing <code>--repo_url</code> with your GitHub fork URL.</p> <pre><code>mlcr push,github,mlperf,inference,submission \\\n   --repo_url=https://github.com/&lt;myfork&gt;/cm4mlperf-inference \\\n   --repo_branch=mlperf-inference-results-scc24 \\\n   --commit_message=\"Results on system &lt;HW Name&gt;\" \\\n   --quiet\n</code></pre> <p>Once uploaded give a Pull Request to the origin repository. Github action will be running there and once finished you can see your submitted results at https://docs.mlcommons.org/cm4mlperf-inference.</p>"},{"location":"benchmarks/language/scc25_guide/scc25/","title":"Text Summarization with Llama2-70b for Student Cluster Competition 2025","text":""},{"location":"benchmarks/language/scc25_guide/scc25/#introduction","title":"Introduction","text":"<p>This guide is designed for the Student Cluster Competition 2025 to walk participants through running and optimizing the MLPerf Inference Benchmark using Llama2 70b across various software and hardware configurations. The goal is to maximize system throughput (measured in Tokens per second) without compromising accuracy. Since the model performs poorly on CPUs, it is essential to run it on GPUs.</p> <p>For a valid MLPerf Inference submission in this competition, you must run both a performance test and an accuracy test\u2014no compliance runs are required. We use the Offline scenario, where throughput is the key metric (higher is better). For Llama 2-70B with the OpenOrca dataset (24,576 samples), the performance run must process an integer multiple of the full dataset (24,576 \u00d7 N samples), while the accuracy run must process exactly the full dataset (24,576 samples). These requirements are taken care of by the MLPerf inference implementations. Setup for NVIDIA GPUs typically takes 2\u20133 hours and can be done offline. The final output is a tarball (<code>mlperf_submission.tar.gz</code>) containing MLPerf-compatible results which can be submitted to the organizers via a CLI command.</p>"},{"location":"benchmarks/language/scc25_guide/scc25/#scoring","title":"Scoring","text":"<p>In the SCC, your first objective will be to get a valid MLPerf benchmark run. Traditionally running the reference MLPerf inference implementation (in Python) is easier compared to running Nvidia MLPerf inference implementation. Since for SCC25 we are having the Llama2-70b model, running the reference implementation needs around 600GB of VRAM and is tested only on 8xH100 Nvidia GPUs. If you have lower VRAM, trying the vendor implementation like of Nvidia or AMD is the best option.  </p> <p>MLCommons provides automation to run the MLPerf inference benchmarks which you can make use of. Currently the automation supports the reference implementation as well as Nvidia implementation and this is useful for you to get a quick valid result as the automation produces the required final output. You can also use the manual steps by following the reference, Nvidia or AMD implementation readmes.</p> <p>Once the initial run is successful, you'll have the opportunity to optimize the benchmark further by maximizing system utilization, applying quantization techniques, adjusting ML frameworks, experimenting with batch sizes, and more, all of which can earn you additional points.</p> <p>Since vendor implementations of the MLPerf inference benchmark vary, teams will compete within their respective hardware categories (e.g., Nvidia GPUs, AMD GPUs). Points will be awarded based on the throughput achieved on your system.</p> <p>Additionally, significant bonus points will be awarded if your team enhances an existing implementation, enables multi-node execution, or adds/extends scripts to mlperf-automations repository supporting new devices, frameworks, implementations etc. All improvements must be made publicly available under the Apache 2.0 license and submitted as pull requests by November 10, 2025 and only the code which is merge ready will be considered for evaluation. As a guideline, below are some examples which can fetch you bonus points. </p> <ul> <li>Adds multi-node execution support for Nvidia, AMD or reference implementations</li> <li>Support automation for AMD implementation</li> <li>Supports fp8/fp4 quantization for Reference implementation</li> <li>Automate the network reference implementation (this uses OpenAI compatible endpoints)</li> <li>The MLPerf automation supports docker run of Nvidia implementation. Supporting apptainer is a valuable contribution</li> </ul> <p>PS: For any query regarding the contribution, feel free to raise an issue in the Inference or MLPerf automations repositories.</p> <p>Info</p> <p>Both MLPerf and MLC automation are evolving projects. If you encounter issues related to SCC, please submit them here with scc-25 label with proper information about the command used, error logs and any additional usefull information to debug the issue.</p> <p>Note: Downloading the models requires service account credentials to be supplied in the run command. These credentials will be shared with participants via their email addresses prior to the start of the competition. Add the following to the existing command described in the sections below: <pre><code>--use_service_account=yes --client_id=&lt;CF-Access-Client-Id&gt; --client_secret=&lt;CF-Access-Client-Secret&gt;\n</code></pre></p>"},{"location":"benchmarks/language/scc25_guide/scc25/#artifacts-to-submit-to-the-scc-committee","title":"Artifacts to submit to the SCC committee","text":"<p>You will need to submit the following files:</p> <ul> <li><code>mlperf_submission.run</code> - MLC commands to run MLPerf inference benchmark saved to this file.</li> <li><code>mlperf_submission.md</code> - description of your platform and some highlights of the MLPerf benchmark execution.</li> <li><code>&lt;Team Name&gt;</code> under which results are pushed to the github repository. </li> </ul>"},{"location":"benchmarks/language/scc25_guide/scc25/#run-commands","title":"Run Commands","text":"MLCommons-PythonNvidia"},{"location":"benchmarks/language/scc25_guide/scc25/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>LLAMA2-70B-99</p> datacenter"},{"location":"benchmarks/language/scc25_guide/scc25/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline scenario and the scenario is  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/scc25_guide/scc25/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/scc25_guide/scc25/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</li> </ul> DockerNative"},{"location":"benchmarks/language/scc25_guide/scc25/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/scc25_guide/scc25/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/scc25_guide/scc25/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/scc25_guide/scc25/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/scc25_guide/scc25/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/scc25_guide/scc25/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/scc25_guide/scc25/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/scc25_guide/scc25/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/scc25_guide/scc25/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/scc25_guide/scc25/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/scc25_guide/scc25/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/scc25_guide/scc25/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/scc25_guide/scc25/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/scc25_guide/scc25/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space:  900GB for manual execution of reference implementation and 1.5TB for automated run through MLC-Scripts</li> </ul> Native"},{"location":"benchmarks/language/scc25_guide/scc25/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/scc25_guide/scc25/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/language/scc25_guide/scc25/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/scc25_guide/scc25/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>LLAMA2-70B-99</p> datacenter"},{"location":"benchmarks/language/scc25_guide/scc25/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline scenario and the scenario is  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/scc25_guide/scc25/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/scc25_guide/scc25/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 2x80GB</p> </li> <li> <p>Disk Space:  900GB for manual execution of vendor implementation and 1.5TB for automated run through MLC-Scripts</p> </li> </ul> Docker"},{"location":"benchmarks/language/scc25_guide/scc25/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/scc25_guide/scc25/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.llama2-70b:1024</code></p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --tp_size=2 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used.</li> <li>Add <code>--adr.llama2-model.tags=_pre-quantized</code> to use the Nvidia quantized models with the available in the MLC Storage. These models were quantized with three different configurations of tensor parallelism and pipeline parallelism: TP1\u2013PP2, TP2\u2013PP1, and TP1\u2013PP1. The appropriate model will be automatically selected based on the values provided for <code>--tp_size</code> and <code>--pp_size</code> in run command. By default tp size of 2 and pp size of 1 would be used. </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for Llama2-70b you can follow this README.</li> </ul>"},{"location":"benchmarks/language/scc25_guide/scc25/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/language/scc25_guide/scc25/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \n</code></pre>"},{"location":"benchmarks/language/scc25_guide/scc25/#submission-commands","title":"Submission Commands","text":""},{"location":"benchmarks/language/scc25_guide/scc25/#generate-actual-submission-tree","title":"Generate actual submission tree","text":"<pre><code>mlcr generate,inference,submission,_wg-inference \\\n   --clean \\\n   --run-checker \\\n   --tar=yes \\\n   --env.MLC_TAR_OUTFILE=submission.tar.gz \\\n   --division=open \\\n   --category=datacenter \\\n   --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n   --quiet \\\n   --submitter=&lt;Team Name&gt;\n</code></pre> <ul> <li>Use <code>--hw_name=\"My system name\"</code> to give a meaningful system name.</li> <li>At the end, a .tar file would be generated inside the current working directory.</li> </ul>"},{"location":"benchmarks/language/scc25_guide/scc25/#submit-results","title":"Submit Results","text":"<p>MLCommons provides students with a Submission UI, where they can upload the generated .tar file using their assigned submission ID.</p> <p>The deadline for submitting results is 6:00 PM CDT on November 17 (Monday), 2025.</p> <p>Alternatively, students may use the Submission CLI provided through the MLCFlow automation. To do this, first follow the installation steps in this guide. After installing, follow the instructions under Upload the final submission.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/","title":"Medical Imaging using 3d-unet (KiTS 2019 kidney tumor segmentation task)","text":"MLCommons-PythonNvidia"},{"location":"benchmarks/medical_imaging/3d-unet/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>3D-UNET-99</p> datacenteredge"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 60GB</li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_1","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 60GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_2","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_3","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 60GB</li> </ul> Native"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_4","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category","title":"Edge category","text":"<p>In the edge category, 3d-unet-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 60GB</li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_12","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_12","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_1","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_13","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_13","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 60GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_14","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_14","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_2","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_15","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_15","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_8","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_16","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_16","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_3","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_17","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_17","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 60GB</li> </ul> Native"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>3D-UNET-99.9</p> datacenteredge"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_9","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_18","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_18","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_4","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_19","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_19","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_2","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 60GB</li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_6","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_10","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_20","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_20","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_5","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_21","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_21","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_7","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_11","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_22","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_22","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_6","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_23","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_23","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 60GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_12","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_24","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_24","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_7","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_25","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_25","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_9","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_13","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_26","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_26","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_8","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_27","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_27","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 60GB</li> </ul> Native"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_14","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_28","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_28","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_9","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_29","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_29","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category_1","title":"Edge category","text":"<p>In the edge category, 3d-unet-99.9 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_3","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 60GB</li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_15","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_30","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_30","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_5","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_31","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_31","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_12","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_16","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_32","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_32","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_6","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_33","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_33","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 60GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_13","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_17","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_34","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_34","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_7","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_35","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_35","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_14","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_18","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_36","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_36","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_8","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_37","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_37","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 60GB</li> </ul> Native"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_15","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for 3d-unet-99.9 you can follow this README.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_19","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_38","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_38","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_9","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_39","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_39","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>3D-UNET-99</p> datacenteredge"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/medical_imaging/3d-unet/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: To be updated</li> </ul> Docker"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.3d-unet:2</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_20","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_40","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_40","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_10","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_41","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_41","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category_2","title":"Edge category","text":"<p>In the edge category, 3d-unet-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/medical_imaging/3d-unet/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: To be updated</li> </ul> Docker"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.3d-unet:2</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>3D-UNET-99.9</p> datacenteredge"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_21","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_42","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_42","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_10","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_43","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_43","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/medical_imaging/3d-unet/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_6","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: To be updated</li> </ul> Docker"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_16","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_22","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_44","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_44","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_11","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_45","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_45","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category_3","title":"Edge category","text":"<p>In the edge category, 3d-unet-99.9 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/medical_imaging/3d-unet/#tensorrt-framework_3","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_7","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: To be updated</li> </ul> Docker"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_17","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_23","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_46","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_46","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_11","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-only_47","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#accuracy-only_47","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/","title":"Medical Imaging using 3d-unet (KiTS 2019 kidney tumor segmentation task)","text":""},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Unprocessed DatasetPreprocessed Dataset ValidationCalibration <p>3d-unet validation run uses the KiTS19 dataset performing KiTS 2019 kidney tumor segmentation task</p> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_KITS19_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>mlcr get,dataset,kits19,_validation -j\n</code></pre>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#get-calibration-dataset","title":"Get Calibration Dataset","text":"<pre><code>mlcr get,dataset,kits19,_calibration -j\n</code></pre>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#get-preprocessed-validation-dataset","title":"Get Preprocessed Validation Dataset","text":"<pre><code>mlcr get,dataset,kits19,preprocessed -j\n</code></pre>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf 3d-unet Model</p> PytorchOnnxTensorflow <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_3DUNET_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#pytorch","title":"Pytorch","text":"<pre><code>mlcr get,ml-model,3d-unet,_pytorch -j\n</code></pre>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#onnx","title":"Onnx","text":"<pre><code>mlcr get,ml-model,3d-unet,_onnx -j\n</code></pre>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#tensorflow","title":"Tensorflow","text":"<pre><code>mlcr get,ml-model,3d-unet,_tensorflow -j\n</code></pre>"},{"location":"benchmarks/object_detection/get-retinanet-data/","title":"Object Detection using Retinanet","text":""},{"location":"benchmarks/object_detection/get-retinanet-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> UnprocessedPreprocessed ValidationCalibration <p>Retinanet validation run uses the OpenImages v6 MLPerf validation dataset resized to 800x800 and consisting of 24,576 images.</p> <p>Retinanet calibration dataset consist of 500 images selected from the OpenImages v6 dataset.</p> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_OPENIMAGES_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/object_detection/get-retinanet-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>mlcr get,dataset,openimages,original,_validation -j\n</code></pre>"},{"location":"benchmarks/object_detection/get-retinanet-data/#get-openimages-calibration-dataset","title":"Get OpenImages Calibration dataset","text":"<pre><code>mlcr get,dataset,openimages,original,_calibration -j\n</code></pre>"},{"location":"benchmarks/object_detection/get-retinanet-data/#get-preprocessed-openimages-dataset","title":"Get Preprocessed OpenImages dataset","text":"<pre><code>mlcr get,dataset,object-detection,open-images,openimages,preprocessed,_validation -j \n</code></pre>"},{"location":"benchmarks/object_detection/get-retinanet-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf Retinanet Model</p> PytorchOnnx <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_RETINANET_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/object_detection/get-retinanet-data/#pytorch","title":"Pytorch","text":"<pre><code>mlcr get,ml-model,retinanet,_pytorch -j\n</code></pre>"},{"location":"benchmarks/object_detection/get-retinanet-data/#onnx","title":"Onnx","text":"<pre><code>mlcr get,ml-model,retinanet,_onnx -j\n</code></pre>"},{"location":"benchmarks/object_detection/get-yolo-data/","title":"Object Detection using Retinanet","text":""},{"location":"benchmarks/object_detection/get-yolo-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Filtered dataset <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_COCO2017_FILTERED_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/object_detection/get-yolo-data/#get-coco-2017-filtered-dataset","title":"Get COCO-2017 filtered Dataset","text":"<pre><code>mlcr get,dataset,mlperf-inference,yolo-coco2017-filtered,_mlc,_r2-downloader -j\n</code></pre>"},{"location":"benchmarks/object_detection/get-yolo-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> Pytorch <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_YOLO_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/object_detection/get-yolo-data/#get-the-official-mlperf-yolo-v11-model","title":"Get the Official MLPerf YOLO v11 Model","text":"<pre><code>mlcr get-ml-model-yolov11,_mlc,_r2-downloader -j\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/","title":"Object Detection using Retinanet","text":"MLCommons-PythonNvidia"},{"location":"benchmarks/object_detection/retinanet/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>RETINANET</p> datacenteredge"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> OnnxruntimePytorch"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_1","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_2","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_3","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"benchmarks/object_detection/retinanet/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_4","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_5","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_12","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_12","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_6","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_13","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_13","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_14","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_14","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_7","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_15","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_15","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_8","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_16","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_16","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_8","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_17","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_17","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"benchmarks/object_detection/retinanet/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_9","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_18","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_18","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_9","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_19","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_19","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#edge-category","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, MultiStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> OnnxruntimePytorch"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework_1","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_10","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_20","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_20","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_21","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_21","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_22","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_22","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_6","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_11","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_23","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_23","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_1","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_24","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_24","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_1","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_25","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_25","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_12","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_26","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_26","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_2","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_27","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_27","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_2","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_28","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_28","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_7","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_13","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_29","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_29","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_3","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_30","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_30","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_3","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_31","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_31","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"benchmarks/object_detection/retinanet/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_8","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_8","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_14","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_32","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_32","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_4","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_33","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_33","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_4","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_34","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_34","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_15","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_35","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_35","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_5","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_36","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_36","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_5","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_37","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_37","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_9","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_9","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_16","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_38","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_38","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_6","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_39","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_39","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_6","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_40","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_40","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_17","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_41","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_41","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_7","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_42","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_42","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_7","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_43","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_43","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_10","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_10","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_18","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_44","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_44","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_8","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_45","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_45","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_8","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_46","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_46","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"benchmarks/object_detection/retinanet/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_11","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_11","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for retinanet you can follow this README.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_19","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_47","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_47","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_9","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_48","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_48","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_9","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_49","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_49","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>RETINANET</p> datacenteredge"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/object_detection/retinanet/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: To be updated</li> </ul> Docker"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_8","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.retinanet:8</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_20","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_50","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_50","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_10","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_51","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_51","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#edge-category_1","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, MultiStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/object_detection/retinanet/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: To be updated</li> </ul> Docker"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_9","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.retinanet:8</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_21","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_52","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_52","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_10","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_53","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_53","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_10","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/retinanet/#performance-only_54","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#accuracy-only_54","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/","title":"Detection using YOLO v11","text":"MLCommons-Python"},{"location":"benchmarks/object_detection/yolo/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>YOLO-95</p> edge"},{"location":"benchmarks/object_detection/yolo/#edge-category","title":"Edge category","text":"<p>In the edge category, yolo-95 has Offline, SingleStream, MultiStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/object_detection/yolo/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDA"},{"location":"benchmarks/object_detection/yolo/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: To be updated</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/yolo/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/yolo/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r6.0-dev \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/yolo/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#singlestream","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#multistream","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_all-scenarios \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/yolo/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r6.0-dev \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/yolo/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#singlestream_1","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#multistream_1","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_all-scenarios \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: To be updated</p> </li> </ul> DockerNative"},{"location":"benchmarks/object_detection/yolo/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/yolo/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r6.0-dev \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/yolo/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#singlestream_2","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#multistream_2","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_all-scenarios \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/object_detection/yolo/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r6.0-dev \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for yolo-95 you can follow this README.</li> </ul> <p>YOLO-99</p> edge"},{"location":"benchmarks/object_detection/yolo/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#singlestream_3","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#multistream_3","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_all-scenarios \\\n   --model=yolo-95 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#edge-category_1","title":"Edge category","text":"<p>In the edge category, yolo-99 has Offline, SingleStream, MultiStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/object_detection/yolo/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDA"},{"location":"benchmarks/object_detection/yolo/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: To be updated</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/yolo/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/yolo/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r6.0-dev \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/yolo/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_12","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_12","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#singlestream_4","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_13","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_13","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#multistream_4","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_14","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_14","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_all-scenarios \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/yolo/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r6.0-dev \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/yolo/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_15","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_15","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#singlestream_5","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_16","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_16","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#multistream_5","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_17","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_17","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_all-scenarios \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: To be updated</p> </li> </ul> DockerNative"},{"location":"benchmarks/object_detection/yolo/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/yolo/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r6.0-dev \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/yolo/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_18","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_18","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#singlestream_6","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_19","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_19","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#multistream_6","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_20","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_20","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_all-scenarios \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/object_detection/yolo/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r6.0-dev \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/object_detection/yolo/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_21","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_21","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#singlestream_7","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_22","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_22","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#multistream_7","title":"MultiStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/object_detection/yolo/#performance-only_23","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_performance-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#accuracy-only_23","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_accuracy-only \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/object_detection/yolo/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r6.0-dev,_all-scenarios \\\n   --model=yolo-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/","title":"Recommendation using DLRM v2","text":"MLCommons-PythonNvidia"},{"location":"benchmarks/recommendation/dlrm-v2/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>DLRM-V2-99</p> datacenteredge"},{"location":"benchmarks/recommendation/dlrm-v2/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/recommendation/dlrm-v2/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/recommendation/dlrm-v2/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_1","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 2x80GB</p> </li> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_2","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_3","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Native"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_4","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#edge-category","title":"Edge category","text":"<p>In the edge category, dlrm-v2-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/recommendation/dlrm-v2/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/recommendation/dlrm-v2/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#singlestream","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_12","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_12","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#singlestream_1","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_13","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_13","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 2x80GB</p> </li> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_14","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_14","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#singlestream_2","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_15","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_15","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_8","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_16","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_16","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#singlestream_3","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_17","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_17","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Native"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>DLRM-V2-99.9</p> datacenteredge"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_9","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_18","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_18","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#singlestream_4","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_19","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_19","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/recommendation/dlrm-v2/#pytorch-framework_2","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/recommendation/dlrm-v2/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_6","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_10","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_20","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_20","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_5","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_21","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_21","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_7","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_11","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_22","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_22","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_6","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_23","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_23","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 2x80GB</p> </li> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_12","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_24","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_24","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_7","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_25","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_25","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_9","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_13","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_26","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_26","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_8","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_27","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_27","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Native"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_14","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_28","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_28","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_9","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_29","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_29","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#edge-category_1","title":"Edge category","text":"<p>In the edge category, dlrm-v2-99.9 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/recommendation/dlrm-v2/#pytorch-framework_3","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/recommendation/dlrm-v2/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_15","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_30","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_30","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#singlestream_5","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_31","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_31","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_12","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_16","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_32","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_32","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#singlestream_6","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_33","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_33","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 2x80GB</p> </li> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_13","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_17","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_34","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_34","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#singlestream_7","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_35","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_35","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_14","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_18","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_36","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_36","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#singlestream_8","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_37","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_37","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Native"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_15","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for dlrm-v2-99.9 you can follow this README.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_19","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_38","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_38","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#singlestream_9","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_39","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_39","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>DLRM-V2-99</p> datacenteredge"},{"location":"benchmarks/recommendation/dlrm-v2/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/recommendation/dlrm-v2/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Docker"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.dlrm-v2:600000</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_20","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_40","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_40","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_10","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_41","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_41","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#edge-category_2","title":"Edge category","text":"<p>In the edge category, dlrm-v2-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/recommendation/dlrm-v2/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Docker"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.dlrm-v2:600000</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>DLRM-V2-99.9</p> datacenteredge"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_21","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_42","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_42","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#singlestream_10","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_43","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_43","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/recommendation/dlrm-v2/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device_6","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Docker"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_16","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_22","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_44","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_44","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_11","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_45","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_45","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#edge-category_3","title":"Edge category","text":"<p>In the edge category, dlrm-v2-99.9 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/recommendation/dlrm-v2/#tensorrt-framework_3","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device_7","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Docker"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_17","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_23","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_46","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_46","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#singlestream_11","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-only_47","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#accuracy-only_47","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt; \n</code></pre>"},{"location":"benchmarks/recommendation/get-dlrm-v2-data/","title":"Recommendation using DLRM v2","text":""},{"location":"benchmarks/recommendation/get-dlrm-v2-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>DLRM validation run uses the Criteo dataset (Day 23).</p> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_CRITEO_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/recommendation/get-dlrm-v2-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>mlcr get,preprocessed,dataset,criteo,_r2-downloader,_mlc -j\n</code></pre>"},{"location":"benchmarks/recommendation/get-dlrm-v2-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf DLRM v2 Model</p> Pytorch <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_DLRM_V2_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/recommendation/get-dlrm-v2-data/#pytorch","title":"Pytorch","text":"<pre><code>mlcr get,ml-model,dlrm,_pytorch,_fp32,_weight_sharded,_r2-downloader -j\n</code></pre>"},{"location":"benchmarks/speech_to_text/get-whisper-data/","title":"Speech to Text using Whisper","text":""},{"location":"benchmarks/speech_to_text/get-whisper-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation PreprocessedUnprocessed"},{"location":"benchmarks/speech_to_text/get-whisper-data/#get-preprocessed-validation-dataset","title":"Get Preprocessed Validation Dataset","text":"<pre><code>mlcr get,dataset,whisper,_preprocessed,_mlc,_r2-downloader --outdirname=&lt;path to download&gt; -j\n</code></pre>"},{"location":"benchmarks/speech_to_text/get-whisper-data/#get-unprocessed-validation-dataset","title":"Get Unprocessed Validation Dataset","text":"<pre><code>mlcr get,dataset,whisper,_unprocessed --outdirname=&lt;path to download&gt; -j\n</code></pre>"},{"location":"benchmarks/speech_to_text/get-whisper-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions if any. In case you want to only download the official model, you can use the below commands.</p> Pytorch From MLCOMMONS <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_WHISPER_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/speech_to_text/get-whisper-data/#get-the-official-mlperf-whisper-model-from-mlcommons-cloudflare-r2","title":"Get the Official MLPerf Whisper model from MLCOMMONS Cloudflare R2","text":"<pre><code>mlcr get,ml-model,whisper,_r2-downloader,_mlc -j\n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/","title":"Speech to Text using Whisper","text":"MLCommons-Python"},{"location":"benchmarks/speech_to_text/whisper/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>WHISPER</p> datacenteredge"},{"location":"benchmarks/speech_to_text/whisper/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, whisper has Offline scenario and the scenario is  mandatory for a closed division submission.</p> vLLM"},{"location":"benchmarks/speech_to_text/whisper/#vllm-framework","title":"vLLM framework","text":"CPUCUDA"},{"location":"benchmarks/speech_to_text/whisper/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: To be updated</li> </ul> DockerNative"},{"location":"benchmarks/speech_to_text/whisper/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/speech_to_text/whisper/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_USE_ML_MODEL_FROM_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_USE_DATASET_FROM_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/speech_to_text/whisper/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/speech_to_text/whisper/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/speech_to_text/whisper/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/speech_to_text/whisper/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/speech_to_text/whisper/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: To be updated</p> </li> </ul> DockerNative"},{"location":"benchmarks/speech_to_text/whisper/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/speech_to_text/whisper/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_USE_ML_MODEL_FROM_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_USE_DATASET_FROM_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/speech_to_text/whisper/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/speech_to_text/whisper/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/speech_to_text/whisper/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/speech_to_text/whisper/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/speech_to_text/whisper/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#edge-category","title":"Edge category","text":"<p>In the edge category, whisper has Offline scenario and the scenario is  mandatory for a closed division submission.</p> vLLM"},{"location":"benchmarks/speech_to_text/whisper/#vllm-framework_1","title":"vLLM framework","text":"CPUCUDA"},{"location":"benchmarks/speech_to_text/whisper/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: To be updated</li> </ul> DockerNative"},{"location":"benchmarks/speech_to_text/whisper/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/speech_to_text/whisper/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_USE_ML_MODEL_FROM_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_USE_DATASET_FROM_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/speech_to_text/whisper/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/speech_to_text/whisper/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/speech_to_text/whisper/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/speech_to_text/whisper/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/speech_to_text/whisper/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: To be updated</p> </li> </ul> DockerNative"},{"location":"benchmarks/speech_to_text/whisper/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/speech_to_text/whisper/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_USE_ML_MODEL_FROM_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_USE_DATASET_FROM_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/speech_to_text/whisper/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/speech_to_text/whisper/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/speech_to_text/whisper/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for whisper you can follow this README.</li> </ul>"},{"location":"benchmarks/speech_to_text/whisper/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/speech_to_text/whisper/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/speech_to_text/whisper/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=whisper \\\n   --implementation=reference \\\n   --framework=vllm \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/text_to_image/get-sdxl-data/","title":"Text to Image using Stable Diffusion","text":""},{"location":"benchmarks/text_to_image/get-sdxl-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration <p>Stable Diffusion validation run uses the Coco 2014 dataset.</p> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_COCO2014_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"benchmarks/text_to_image/get-sdxl-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>mlcr get,dataset,coco2014,_validation -j\n</code></pre>"},{"location":"benchmarks/text_to_image/get-sdxl-data/#get-coco2014-calibration-dataset","title":"Get COCO2014 Calibration Dataset","text":"<pre><code>mlcr get,dataset,coco2014,_calibration -j\n</code></pre>"},{"location":"benchmarks/text_to_image/get-sdxl-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf Stable Diffusion Model</p> Pytorch FP 16FP 32 <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_SDXL_MODEL&gt;</code> could be provided to download the model to a specific location.</li> </ul>"},{"location":"benchmarks/text_to_image/get-sdxl-data/#pytorch","title":"Pytorch","text":"<pre><code>mlcr get,ml-model,sdxl,_pytorch,_fp16,_r2-downloader -j\n</code></pre>"},{"location":"benchmarks/text_to_image/get-sdxl-data/#pytorch_1","title":"Pytorch","text":"<pre><code>mlcr get,ml-model,sdxl,_pytorch,_fp32,_r2-downloader -j\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/","title":"Text to Image using Stable Diffusion","text":"MLCommons-PythonNvidia"},{"location":"benchmarks/text_to_image/sdxl/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>Note: SDXL reference implementation does not support multithreading.</p> <p>SDXL</p> datacenteredge"},{"location":"benchmarks/text_to_image/sdxl/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/text_to_image/sdxl/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/text_to_image/sdxl/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server_1","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB(fp32), 16GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_4","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_4","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server_2","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_5","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_5","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_6","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_6","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server_3","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_7","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_7","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/text_to_image/sdxl/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_4","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_8","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_8","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server_4","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_9","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_9","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#edge-category","title":"Edge category","text":"<p>In the edge category, sdxl has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/text_to_image/sdxl/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/text_to_image/sdxl/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_5","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_10","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_10","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_11","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_11","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_6","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_12","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_12","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_1","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_13","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_13","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB(fp32), 16GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_7","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_14","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_14","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_2","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_15","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_15","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_8","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_16","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_16","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_3","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_17","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_17","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/text_to_image/sdxl/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10 --rerun\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for sdxl you can follow this README.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_9","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_18","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_18","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_4","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_19","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_19","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>SDXL</p> datacenteredge"},{"location":"benchmarks/text_to_image/sdxl/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/text_to_image/sdxl/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/text_to_image/sdxl/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: 16GB</li> </ul> Docker"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.sdxl:clip1:1##clip2:2##unet:2##vae:1</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_10","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_20","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_20","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#server_5","title":"Server","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_21","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_21","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#edge-category_1","title":"Edge category","text":"<p>In the edge category, sdxl has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/text_to_image/sdxl/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/text_to_image/sdxl/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: 16GB</li> </ul> Docker"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.sdxl:clip1:1##clip2:2##unet:2##vae:1</code></p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>If you encounter an error related to ulimit or max locked memory during the run_harness step, please refer to the this issue for details and resolution steps.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_11","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_22","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_22","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_5","title":"SingleStream","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/sdxl/#performance-only_23","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_performance-only \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#accuracy-only_23","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>mlcr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/","title":"Text-to-Image with Stable Diffusion for Student Cluster Competition 2024","text":""},{"location":"benchmarks/text_to_image/reproducibility/scc24/#introduction","title":"Introduction","text":"<p>This guide is designed for the Student Cluster Competition 2024 to walk participants through running and optimizing the MLPerf Inference Benchmark using Stable Diffusion XL 1.0 across various software and hardware configurations. The goal is to maximize system throughput (measured in samples per second) without compromising accuracy. Since the model performs poorly on CPUs, it is essential to run it on GPUs.</p> <p>For a valid MLPerf inference submission, two types of runs are required: a performance run and an accuracy run. In this competition, we focus on the <code>Offline</code> scenario, where throughput is the key metric\u2014higher values are better. The official MLPerf inference benchmark for Stable Diffusion XL requires processing a minimum of 5,000 samples in both performance and accuracy modes using the COCO 2014 dataset. However, for SCC, we have reduced this and we also have two variants. <code>scc-base</code> variant has dataset size reduced to 50 samples, making it possible to complete both performance and accuracy runs in approximately 5-10 minutes. <code>scc-main</code> variant has dataset size of 500 and running it will fetch extra points as compared to running just the base variant. Setting up for Nvidia GPUs may take 2-3 hours but can be done offline. Your final output will be a tarball (<code>mlperf_submission.tar.gz</code>) containing MLPerf-compatible results, which you will submit to the SCC organizers for scoring.</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#scoring","title":"Scoring","text":"<p>In the SCC, your first objective will be to run <code>scc-base</code> variant for reference (unoptimized) Python implementation or a vendor-provided version (such as Nvidia's) of the MLPerf inference benchmark to secure a baseline score.</p> <p>Once the initial run is successful, you'll have the opportunity to optimize the benchmark further by maximizing system utilization, applying quantization techniques, adjusting ML frameworks, experimenting with batch sizes, and more, all of which can earn you additional points.</p> <p>Since vendor implementations of the MLPerf inference benchmark vary and are often limited to single-node benchmarking, teams will compete within their respective hardware categories (e.g., Nvidia GPUs, AMD GPUs). Points will be awarded based on the throughput achieved on your system.</p> <p>Additionally, significant bonus points will be awarded if your team enhances an existing implementation, adds support for new hardware (such as an unsupported GPU), enables multi-node execution, or adds/extends scripts to cm4mlops repository supporting new devices, frameworks, implementations etc. All improvements must be made publicly available under the Apache 2.0 license and submitted alongside your results to the SCC committee to earn these bonus points, contributing to the MLPerf community.</p> <p>Info</p> <p>Both MLPerf and MLC automation are evolving projects. If you encounter issues or have questions, please submit them here</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#artifacts-to-submit-to-the-scc-committee","title":"Artifacts to submit to the SCC committee","text":"<p>You will need to submit the following files:</p> <ul> <li><code>mlperf_submission.run</code> - MLC commands to run MLPerf inference benchmark saved to this file.</li> <li><code>mlperf_submission.md</code> - description of your platform and some highlights of the MLPerf benchmark execution.</li> <li><code>&lt;Team Name&gt;</code> under which results are pushed to the github repository. </li> </ul>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#scc-interview","title":"SCC interview","text":"<p>You are encouraged to highlight and explain the obtained MLPerf inference throughput on your system and describe any improvements and extensions to this benchmark (such as adding new hardware backend or supporting multi-node execution) useful for the community and MLCommons.</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#run-commands","title":"Run Commands","text":"MLCommons-PythonNvidia"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>Note: SDXL reference implementation does not support multithreading.</p> <p>SDXL</p> datacenter"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline scenario and the scenario is  mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#pytorch-framework","title":"Pytorch framework","text":"ROCmCUDA"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_r5.1-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n    --precision=float16 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#offline","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#performance-only","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_r5.1-dev,_short,_scc24-base,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm \\\n   --quiet --precision=float16 --rerun\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#accuracy-only","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_r5.1-dev,_short,_scc24-base,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm \\\n   --quiet --precision=float16 --rerun\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB(fp32), 16GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_r5.1-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n    --precision=float16 --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#offline_1","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#performance-only_1","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_r5.1-dev,_short,_scc24-base,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda \\\n   --quiet --precision=float16 --rerun\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#accuracy-only_1","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_r5.1-dev,_short,_scc24-base,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda \\\n   --quiet --precision=float16 --rerun\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. </p> </li> <li> <p><code>_r5.1-dev</code> could also be given instead of <code>_r6.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_r5.1-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n    --precision=float16 --rerun\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#offline_2","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#performance-only_2","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_r5.1-dev,_short,_scc24-base,_performance-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda \\\n   --quiet --precision=float16 --rerun\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#accuracy-only_2","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_r5.1-dev,_short,_scc24-base,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda \\\n   --quiet --precision=float16 --rerun\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>SDXL</p> datacenter"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline scenario and the scenario is  mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Device Memory: 16GB</li> </ul> Docker"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size. Example: <code>--batch_size.sdxl:clip1:1##clip2:2##unet:2##vae:1</code></p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <p><pre><code>mlcr run-mlperf,inference,_find-performance,_r5.0-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet --rerun\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_privileged</code>: to launch the container in privileged mode</p> </li> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned mlperf-automations repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>Info</p> <p>Once the above run is successful, you can change <code>_scc24-base</code> to <code>_scc24-main</code> to run the main variant.</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#offline_3","title":"Offline","text":"performance-onlyaccuracy-only"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#performance-only_3","title":"performance-only","text":"<pre><code>mlcr run-mlperf,inference,_r5.0-dev,_short,_scc24-base,_performance-only \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda \\\n   --quiet  --rerun\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#accuracy-only_3","title":"accuracy-only","text":"<pre><code>mlcr run-mlperf,inference,_r5.0-dev,_short,_scc24-base,_accuracy-only \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda \\\n   --quiet  --rerun\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#submission-commands","title":"Submission Commands","text":""},{"location":"benchmarks/text_to_image/reproducibility/scc24/#generate-actual-submission-tree","title":"Generate actual submission tree","text":"<pre><code>mlcr generate,inference,submission \\\n   --clean \\\n   --run-checker \\\n   --tar=yes \\\n   --env.CM_TAR_OUTFILE=submission.tar.gz \\\n   --division=open \\\n   --category=datacenter \\\n   --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n   --run_style=test \\\n   --adr.submission-checker.tags=_short-run \\\n   --quiet \\\n   --submitter=&lt;Team Name&gt;\n</code></pre> <ul> <li>Use <code>--hw_name=\"My system name\"</code> to give a meaningful system name.</li> </ul>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#push-results-to-github","title":"Push Results to GitHub","text":"<p>Fork the <code>mlperf-inference-results-scc24</code> branch of the repository URL at https://github.com/mlcommons/cm4mlperf-inference. </p> <p>Run the following command after replacing <code>--repo_url</code> with your GitHub fork URL.</p> <pre><code>mlcr push,github,mlperf,inference,submission \\\n   --repo_url=https://github.com/&lt;myfork&gt;/cm4mlperf-inference \\\n   --repo_branch=mlperf-inference-results-scc24 \\\n   --commit_message=\"Results on system &lt;HW Name&gt;\" \\\n   --quiet\n</code></pre> <p>Once uploaded give a Pull Request to the origin repository. Github action will be running there and once  finished you can see your submitted results at https://docs.mlcommons.org/cm4mlperf-inference.</p>"},{"location":"changelog/","title":"What's New &amp; What's Coming \ud83d\ude80","text":"<p>Info</p> <p>Inference v5.0 Submission is approaching! The submission deadline is February 28, 2025, at 1 PM PST.  </p> <p>Tip</p> <p>Starting January 2025, MLPerf Inference automations are powered by MLCFlow\u2014a newly developed Python package that replaces the previously used CMind package. This transition enhances automation, streamlines workflows, and makes MLPerf scripts more independent and simpler.  </p>"},{"location":"changelog/changelog/","title":"Release Notes","text":"<p>\ud83d\ude80 mlc-scripts 1.0.0 was released on February 10, 2025, introducing full support for MLPerf Inference v5.0 using the first stable release of MLCFlow 1.0.1.  </p> <p>\ud83d\udd39 All previous CM scripts used in MLPerf have been successfully ported to the MLC interface, ensuring seamless integration. Additionally, all GitHub Actions are now passing, confirming a stable and reliable implementation.  </p>"},{"location":"changelog/changelog/#key-updates-in-mlcflow","title":"Key Updates in MLCFlow","text":"<p>\u2705 Simplified Interface - A redesigned approach using Actions and Targets, making the CLI more intuitive for users.  </p> <p>\u2705 Unified Automation Model - Consolidated into a single automation entity: Script, which is seamlessly extended by Cache, Docker, and Tests.  </p> <p>\u2705 Improved Docker Integration - A cleaner, more efficient Docker extension, streamlining containerized execution.  </p> <p>\u2705 Enhanced Script Management - Tighter integration between the interface and script automation, making script creation and management easier than ever.  </p>"},{"location":"demos/","title":"Demos","text":""},{"location":"install/","title":"Installation","text":"<p>We use MLCommons MLC Automation framework to run MLPerf inference benchmarks.</p> <p>MLC needs <code>git</code>, <code>python3-pip</code> and <code>python3-venv</code> installed on your system. Once the dependencies are installed, do the following</p>"},{"location":"install/#activate-a-virtual-env-for-mlcflow","title":"Activate a Virtual ENV for MLCFlow","text":"<p>This step is not mandatory as MLC can use separate virtual environment for MLPerf inference. But the latest <code>pip</code> install requires this or else will need the <code>--break-system-packages</code> flag while installing <code>mlc-scripts</code>.</p> <pre><code>python3 -m venv mlc\nsource mlc/bin/activate\n</code></pre>"},{"location":"install/#install-mlc-and-pulls-any-needed-repositories","title":"Install MLC and pulls any needed repositories","text":"Use the default fork of MLC-Scripts repositoryUse custom fork/branch of the MLC-Scripts repository <pre><code> pip install mlc-scripts\n</code></pre> <p><pre><code> pip install mlcflow &amp;&amp; mlc pull repo --url=mlcommons@mlperf-automations --branch=dev\n</code></pre> Here, <code>repo</code> is in the format <code>githubUsername@githubRepo</code> or you can give any URL</p> <p>Now, you are ready to use the <code>mlcr</code> commands to run MLPerf inference as given in the benchmarks page</p>"},{"location":"power/","title":"Power Measurement","text":"<p>Originally Prepared by the MLCommons taskforce on automation and reproducibility and OctoML.</p>"},{"location":"power/#requirements","title":"Requirements","text":"<ol> <li> <p>Power analyzer (anyone certified by SPEC PTDaemon). Yokogawa is the one that most submitters have submitted with and a new single-channel model like 310E can cost around 3000$. </p> </li> <li> <p>SPEC PTDaemon (can be downloaded from here after signing the EULA which can be requested by sending an email to <code>support@mlcommons.org</code>). Once you have GitHub access to the MLCommons power repository then the MLC workflow will automatically download and configure the SPEC PTDaemon tool.</p> </li> <li> <p>Access to the MLCommons power-dev repository which has the <code>server.py</code> to be run on the director node and <code>client.py</code> to be run on the SUT node. This repository being public will be automatically pulled by the MLC workflow.</p> </li> </ol>"},{"location":"power/#connecting-power-analyzer-to-the-computer","title":"Connecting power analyzer to the computer","text":"<p>We need to connect the power analyzer to a director machine via USB if the director machine is running Linux. Ethernet and serial modes are supported only on Windows. The power supply to the SUT is done through the power analyzer (current in series and voltage in parallel). An adapter like this can help avoid cutting the electrical wires. </p> <p>.</p> <p>The director machine runs the <code>server.py</code> script and loads a server process that communicates with the SPEC PTDaemon. When a client connects to it (using <code>client.py</code>), it in turn connects to the PTDaemon and initiates a measurement run. Once the measurement ends, the power log files are transferred to the client. </p>"},{"location":"power/#ranging-mode-and-testing-mode","title":"Ranging mode and Testing mode","text":"<p>Power analyzers usually have different current and voltage ranges it supports and the exact ranges to be used depends on a given SUT and this needs some empirical data. We can do a ranging run where the current and voltage ranges are set to <code>Auto</code> and the power analyzer automatically figures out the correct ranges needed. These determined ranges are then used for a proper testing mode run. Using the 'auto' mode in a testing run is not allowed as it can mess up the measurements.</p>"},{"location":"power/#start-power-server-power-analyzer-should-be-connected-to-this-computer-and-ptdaemon-runs-here","title":"Start Power Server (Power analyzer should be connected to this computer and PTDaemon runs here)","text":"<p>If you are having GitHub access to MLCommons power repository, PTDaemon should be automatically installed using the below MLC command:</p> <p>PS: The below command will ask for <code>sudo</code> permission on Linux and should be run with administrator privilege on Windows (to do NTP time sync). <pre><code>mlcr mlperf,power,server --device_type=49 --device_port=/dev/usbtmc0\n</code></pre> * <code>`--interface_flag=\"-U\" and</code>--device_port=1<code>(can change as per the USB slot used for connecting) can be used on Windows for USB connection *</code>--device_type=49<code>corresponds to Yokogawa 310E and</code>ptd -h<code>should list the device_type for all supported devices. The location of</code>ptd<code>can be found using the below command *</code>--device_port=20<code>and</code>--interface_flag=\"-g\" can be used to connect to GPIB interface (currently supported only on Windows) with the serial address set to 20 <pre><code>cat `mlc find cache --tags=get,spec,ptdaemon`/mlc-cached-state.json\n</code></pre></p> <p>An example analyzer configuration file <pre><code>[server]\nntpserver = time.google.com\nlisten = 0.0.0.0 4950\n\n[ptd]\nptd = C:\\Users\\arjun\\CM\\repos\\local\\cache\\5a0a52d578724774\\repo\\PTD\\binaries\\ptd-windows-x86.exe\nanalyzerCount = 2\n[analyzer2]\ninterfaceflag = -g\ndevicetype = 8\ndeviceport = 20\nnetworkport = 8888\n\n[analyzer1]\ninterfaceflag = -y\ndevicetype = 49\ndeviceport = C3YD21068E\nnetworkport = 8889\n</code></pre></p>"},{"location":"power/#running-the-power-server-inside-a-docker-container","title":"Running the power server inside a docker container","text":"<p><pre><code>mlc docker script --tags=run,mlperf,power,server --docker_gh_token=&lt;GITHUB AUTH_TOKEN&gt; \\\n--device=/dev/usbtmc0\n</code></pre> * Device address may need to be changed depending on the USB port being used * The above command uses a host-container port mapping 4950:4950 which can be changed by using <code>--docker_port_maps,=4950:4950</code></p>"},{"location":"power/#running-a-dummy-workload-with-power-on-host-machine","title":"Running a dummy workload with power (on host machine)","text":"<pre><code>mlcr mlperf,power,client --power_server=&lt;POWER_SERVER_IP&gt; \n</code></pre>"},{"location":"power/#run-a-dummy-workload-with-power-inside-a-docker-container","title":"Run a dummy workload with power inside a docker container","text":"<pre><code>mlc docker script --tags==mlperf,power,client --power_server=&lt;POWER_SERVER_IP&gt;\"\n</code></pre>"},{"location":"power/#running-mlperf-image-classification-with-power","title":"Running MLPerf Image Classification with power","text":"<pre><code>mlcr app,mlperf,inference,_reference,_power,_resnet50,_onnxruntime,_cpu --mode=performance --power_server=&lt;POWER_SERVER_IP&gt;\n</code></pre>"},{"location":"power/#running-mlperf-image-classification-with-power-inside-a-docker-container","title":"Running MLPerf Image Classification with power inside a docker container","text":"<pre><code>mlcr app,mlperf,inference,_reference,_power,_resnet50,_onnxruntime,_cpu --mode=performance --power_server=&lt;POWER_SERVER_IP&gt; --docker\n</code></pre>"},{"location":"submission/","title":"MLPerf Inference Submission Guide","text":"<p>This document provides a step-by-step overview of the MLPerf Inference submission process.  </p> <pre><code>\n%%{init: {\"themeVariables\": { \"fontSize\": \"12px\" }}}%%\nflowchart TD\n    A([Start]) --&gt; B[Prerequisites &amp; Agreements]\n\n    %% Setup phase\n    B --&gt; C[Set up Benchmark Environment]\n\n     %% Iterative benchmarking loop\n    subgraph L[For Each Benchmark]\n        direction TB\n        C --&gt; D[Download Assets]\n        D --&gt; E[Run Benchmarks]\n        E --&gt; F[Validate Accuracy]\n        F --&gt; G{More Benchmarks to Run?}\n        G --&gt;|Yes| C\n    end\n\n    %% Submission preparation\n    G --&gt;|No| H[Prepare Results Structure]\n    H --&gt; I[Run Submission Checker]\n    I --&gt; J[Generate Final Tarball]\n    J --&gt; K[Upload via Submission UI]\n    K --&gt; L1([End])</code></pre>"},{"location":"submission/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Prerequisites and Agreements</li> <li>2. Overview of MLPerf Inference Benchmarking</li> <li>3. Common Steps in Benchmarking</li> <li>4. Preparing Results for Submission</li> <li>5. Manual vs Automated Submission</li> <li>6. Additional Resources</li> <li>7. Troubleshooting</li> <li>8. Support</li> <li>Notes</li> </ul>"},{"location":"submission/#1-prerequisites-and-agreements","title":"1. Prerequisites and Agreements","text":"<p>For doing the MLPerf Inference Submission, ensure that your company is a member of MLCommons and all the required agreements are completed. You can skip these if you are doing an unofficial submission.</p> <ul> <li> <p>Becoming a member:</p> <ul> <li>Please find more information about becoming a MLCommons member here.</li> </ul> </li> <li> <p>Sign the relevant agreements:</p> <ul> <li>Contributor License Agreement (CLA)</li> <li>Trademark Agreement (Contact <code>support@mlcommons.org</code> to request the trademark agreement.)</li> <li>Power EULA (optional, only if submitting power measurements which can be requested by sending an email to <code>support@mlcommons.org</code>)</li> </ul> </li> </ul> <p>The submitting organizations/individuals are obliged to follow the below guidelines:</p> <ul> <li>Results Guidelines</li> <li>Trademark Guidelines</li> </ul>"},{"location":"submission/#2-overview-of-mlperf-inference-benchmarking","title":"2. Overview of MLPerf Inference Benchmarking","text":"<p>MLPerf Inference is a benchmarking suite designed to measure performance of machine learning models on different hardware and software systems.</p>"},{"location":"submission/#benchmark-timings","title":"Benchmark Timings","text":"<ul> <li> <p>MLPerf Inference benchmarking happens twice per year:</p> <ul> <li>.0 Round \u2013 February  </li> <li>.1 Round \u2013 August  </li> </ul> </li> </ul>"},{"location":"submission/#categories","title":"Categories","text":"<p>Results are grouped into categories based on system availability:</p> <ul> <li>Available: All components are available for purchase or rent.  </li> <li>Preview: Expected to become Available in the next submission round.  </li> <li>Research, Development, or Internal (RDI): Experimental or internal systems.</li> </ul>"},{"location":"submission/#system-types","title":"System Types","text":"<p>Two system types are supported:</p> <ul> <li>Datacenter: Large-scale server-class systems.  </li> <li>Edge: Devices and platforms designed for embedded or low-power environments.</li> </ul> <p>The System Under Test (SUT) can be classified under either, depending on the use case.</p>"},{"location":"submission/#scenarios","title":"Scenarios","text":"<p>Each benchmark defines one or more scenarios to reflect real-world usage. See more details here</p> <ul> <li>Datacenter: Offline, Server, Interactive  </li> <li>Edge: SingleStream, MultiStream, Offline  </li> </ul> <p>Scenario requirements depend on both system type and benchmark model. Refer to the Benchmark Info page for details.</p>"},{"location":"submission/#divisions","title":"Divisions","text":"<p>MLPerf supports two divisions, allowing flexibility in reimplementation:</p> <ul> <li> <p>Closed Division:   Focused on fair \u201capples-to-apples\u201d comparison using the same model and reference setup. All applicable scenarios for a given benchmark are mandatory to be submitted.    See rules.</p> </li> <li> <p>Open Division:   Allows innovation such as retraining or model substitution. One or more scenario of any benchmark can be submitted.   See rules.</p> </li> </ul>"},{"location":"submission/#loadgen-load-generator","title":"LoadGen (Load Generator)","text":"<p>LoadGen is the C++ benchmarking harness (with Python bindings) used across all MLPerf Inference submissions. It handles:</p> <ul> <li>Query generation and scheduling </li> <li>logging  </li> <li>Latency tracking  </li> <li>Accuracy validation  </li> <li>Final metric computation  </li> </ul>"},{"location":"submission/#3-common-steps-in-benchmarking","title":"3. Common Steps in Benchmarking","text":"<p>Below are the general steps followed in the MLPerf Inference benchmarking process.</p> <ol> <li> <p>Determine Division, System Type, and Category    Decide whether your submission will be Closed/Open, Edge/Datacenter, and Available/Preview/RDI.</p> </li> <li> <p>Identify Required Scenarios    Based on your chosen configuration, identify which scenarios must be run.</p> </li> <li> <p>Download Required Resources</p> <ul> <li> <p>Benchmark Source Repository (Reference or vendor implementation):  </p> </li> <li> <p>Datasets </p> </li> <li> <p>Models </p> </li> </ul> </li> <li> <p>Configure Environment    Set up the required dependencies, environment variables, and system configurations for benchmarking.</p> <p>Please refer to this README for instructions on how to automatically generate the system description file for your SUT.</p> </li> <li> <p>Run Benchmarks    Execute benchmarks for the required scenarios, divisions, and categories.    Adjust target QPS or latency to meet accuracy and performance requirements. (Refer to model-specific documentations for actual command usage.)</p> </li> <li> <p>Run Accuracy Checker    Verify that the output results meet accuracy thresholds.  </p> </li> <li> <p>Repeat for All Intended Benchmarks    Perform the above steps for every model you plan to submit.</p> </li> </ol>"},{"location":"submission/#4-preparing-results-for-submission","title":"4. Preparing Results for Submission","text":"<p>After benchmark runs are complete, follow these steps to prepare your submission package.</p> <ol> <li> <p>Arrange Results    Organize results according to the structure defined here.</p> </li> <li> <p>Preprocess Submission    Use the preprocessing script from the inference repository to validate and format results.</p> </li> <li> <p>Run Submission Checker    The checker validates completeness and correctness, producing a final <code>.tar.gz</code> file for submission.</p> </li> <li> <p>Upload to Submission Portal    Use the MLCommons Submission UI, the link to which will be shared at the time of submission, to upload your final tarball.</p> </li> </ol>"},{"location":"submission/#5-manual-vs-automated-submission","title":"5. Manual vs Automated Submission","text":"<p>There are two ways to prepare and submit your results:</p>"},{"location":"submission/#manual-submission","title":"Manual Submission","text":"<p>Each of the above steps (from benchmarking to submission) can be performed manually. This approach gives full control but is time-consuming and prone to errors.</p>"},{"location":"submission/#automated-submission-cli","title":"Automated Submission (CLI)","text":"<p>MLCommons provides a Submission CLI that automates:</p> <ul> <li>Result structuring</li> <li>Preprocessing</li> <li>Checker validation</li> <li>Final tarball creation and upload</li> </ul> <p>This reduces manual intervention and ensures compliance with submission requirements. See documentation for usage.</p>"},{"location":"submission/#6-additional-resources","title":"6. Additional Resources","text":"<ul> <li>MLPerf General Policies</li> <li>MLPerf Inference Policies </li> <li>Reference Implementations </li> <li>Previous Submission Results</li> <li>Submission FAQ</li> </ul>"},{"location":"submission/#7-troubleshooting","title":"7. Troubleshooting","text":""},{"location":"submission/#8-support","title":"8. Support","text":"<p>For questions or clarifications:</p> <ul> <li>Join the MLPerf Inference mailing list \u2014 you can contact the group via inference@mlcommons.org.</li> <li>Open an issue in the inference repository.</li> </ul>"},{"location":"submission/#notes","title":"Notes","text":"<ul> <li>Ensure that all agreements and contributor information are completed before performing any submissions.  </li> <li>Always use the latest round\u2019s branch/tag for submission.  </li> <li>Refer to individual model documentation for scenario-specific configurations.  </li> </ul> <p>This document is maintained by the MLPerf Inference Working Group. For suggestions or updates, please raise a pull request or contact the maintainers.</p>"},{"location":"submission/submission-cli/","title":"Automated Submission CLI","text":"<p>Click here to view the proposal slide for Common Automation for MLPerf Inference Submission Generation through MLCFlow.</p> <p>Please refer to the installation page to install MLCFlow for automating the submission generation. In a typical development environment <code>pip install mlc-scripts</code> should be enough.</p> Custom automation based MLPerf resultsMLC automation based results <p>If you have not followed the <code>mlcr</code> commands under the individual model pages in the benchmarks directory, please make sure that the result directory is structured in the following way. You can see the real examples for the expected folder structure here. <pre><code>\u2514\u2500\u2500 System description ID(SUT Name)\n    \u251c\u2500\u2500 system_meta.json\n    \u2514\u2500\u2500 Benchmark\n        \u2514\u2500\u2500 Scenario\n            \u251c\u2500\u2500 Performance\n            |   \u2514\u2500\u2500 run_1 run for all scenarios\n            |       \u251c\u2500\u2500 mlperf_log_summary.txt\n            |       \u2514\u2500\u2500 mlperf_log_detail.txt\n            \u251c\u2500\u2500 Accuracy\n            |   \u251c\u2500\u2500 mlperf_log_summary.txt\n            |   \u251c\u2500\u2500 mlperf_log_detail.txt\n            |   \u251c\u2500\u2500 mlperf_log_accuracy.json\n            |   \u2514\u2500\u2500 accuracy.txt\n            |\u2500\u2500 Compliance_Test_ID\n            |   \u251c\u2500\u2500 Performance\n            |   |   \u2514\u2500\u2500 run_x/#1 run for all scenarios\n            |   |       \u251c\u2500\u2500 mlperf_log_summary.txt\n            |   |       \u2514\u2500\u2500 mlperf_log_detail.txt\n            |   \u251c\u2500\u2500 Accuracy # for TEST01 only\n            |   |   \u251c\u2500\u2500 baseline_accuracy.txt (if test fails in deterministic mode)\n            |   |   \u251c\u2500\u2500 compliance_accuracy.txt (if test fails in deterministic mode)\n            |   |   \u251c\u2500\u2500 mlperf_log_accuracy.json\n            |   |   \u2514\u2500\u2500 accuracy.txt\n            |   \u251c\u2500\u2500 verify_performance.txt\n            |   \u2514\u2500\u2500 verify_accuracy.txt # for TEST01 only\n            |\u2500\u2500 user.conf\n            \u2514\u2500\u2500 measurements.json\n</code></pre></p> <p> Click here if you are submitting in open division <ul> <li>The <code>model_mapping.json</code> should be included inside the SUT folder which is used to map the custom model full name to the official model name. The format of json file is:</li> </ul> <p><pre><code>    {\n        \"custom_model_name_for_model1\":\"official_model_name_for_model1\",\n        \"custom_model_name_for_model2\":\"official_model_name_for_model2\",\n\n    }\n</code></pre> </p> <p>If you have followed the <code>mlcr</code> commands under the individual model pages in the benchmarks directory, all the valid results will get aggregated to the <code>mlc cache</code> folder. The following command could be used to browse the structure of inference results folder generated by MLCFlow.</p> <p>Once all the results across all the models are ready you can use the following the below section to generate a valid submission tree compliant with the MLPerf requirements.</p>"},{"location":"submission/submission-cli/#get-results-folder-structure","title":"Get results folder structure","text":"Unix TerminalWindows PowerShell <pre><code>mlc find cache --tags=get,mlperf,inference,results,dir | xargs tree\n</code></pre> <pre><code>mlc find cache --tags=get,mlperf,inference,results,dir |  ForEach-Object { Get-ChildItem -Recurse $_ }\n</code></pre>"},{"location":"submission/submission-cli/#generate-submission-folder","title":"Generate submission folder","text":"<p>The submission generation flow is explained in the below diagram</p> <pre><code>flowchart LR\n    subgraph Generation [Submission Generation SUT1]\n      direction TB\n      A[populate system details] --&gt; B[generate submission structure]\n      B --&gt; C[truncate-accuracy-logs]\n      C --&gt; D{Infer low latency results &lt;br&gt;and/or&lt;br&gt; filter out invalid results}\n      D --&gt; yes --&gt; E[preprocess-mlperf-inference-submission]\n      D --&gt; no --&gt; F[run-mlperf-inference-submission-checker]\n      E --&gt; F\n    end\n    Input((Results SUT1)) --&gt; Generation\n    Generation --&gt; Output((Submission Folder &lt;br&gt; SUT1))</code></pre>"},{"location":"submission/submission-cli/#command-to-generate-submission-folder","title":"Command to generate submission folder","text":"<pre><code>mlcr generate,inference,submission \\\n  --clean \\\n  --preprocess_submission=yes \\\n  --run_checker=yes \\\n  --submitter=MLCommons \\\n  --division=closed \\\n  --env.MLC_DETERMINE_MEMORY_CONFIGURATION=yes \\\n  --quiet\n</code></pre> <p>Tip</p> <ul> <li> <p>Use <code>--hw_name=\"My system name\"</code> to give a meaningful system name. Examples can be seen here</p> </li> <li> <p>Use <code>--submitter=&lt;Your name&gt;</code> if your organization is an official MLCommons member and would like to submit under your organization</p> </li> <li> <p>Use <code>--hw_notes_extra</code> option to add additional notes like <code>--hw_notes_extra=\"Result taken by NAME\"</code></p> </li> <li> <p>Use <code>--results_dir</code> option to specify the results folder.  It is automatically taken from MLC cache for MLPerf automation based runs</p> </li> <li> <p>Use <code>--submission_dir</code> option to specify the submission folder. (You can avoid this if you're pushing to github or only running a single SUT and MLC will use its cache folder)</p> </li> <li> <p>Use <code>--division=open</code> for open division submission </p> </li> <li> <p>Use <code>--category</code> option to specify the category for which submission is generated(datacenter/edge). By default, the category is taken from <code>system_meta.json</code> file located in the SUT root directory.</p> </li> <li> <p>Use <code>--submission_base_dir</code> to specify the directory to which the outputs from preprocess submission script and final submission is added. No need to provide <code>--submission_dir</code> along with this. For <code>docker run</code>, use <code>--submission_base_dir</code> instead of <code>--submission_dir</code>.</p> </li> </ul> <p>If there are multiple systems where MLPerf results are collected, the same process needs to be repeated on each of them. One we have submission folders on all the SUTs, we need to sync them to make a single submission folder</p> Sync LocallySync via a Github repo <p>If you are having results in multiple systems, you need to merge them to one system. You can use <code>rsync</code> for this. For example, the below command will sync the submission folder from SUT2 to the one in SUT1.  <pre><code>rsync -avz username@host1:&lt;path_to_submission_folder2&gt;/ &lt;path_to_submission_folder1&gt;/\n</code></pre> Same needs to be repeated for all other SUTs so that we have the full submissions in SUT1.</p> <pre><code>    flowchart LR\n        subgraph SUT1 [Submission Generation SUT1]\n          A[Submission Folder SUT1]\n        end\n        subgraph SUT2 [Submission Generation SUT2]\n          B[Submission Folder SUT2]\n        end\n        subgraph SUT3 [Submission Generation SUT3]\n          C[Submission Folder SUT3]\n        end\n        subgraph SUTN [Submission Generation SUTN]\n          D[Submission Folder SUTN]\n        end\n        SUT2 --&gt; SUT1\n        SUT3 --&gt; SUT1\n        SUTN --&gt; SUT1\n</code></pre> <p>If you are collecting results across multiple systems you can generate different submissions and aggregate all of them to a GitHub repository (can be private) and use it to generate a single tar ball which can be uploaded to the MLCommons Submission UI. </p> <p>Run the following command after replacing <code>--repo_url</code> with your GitHub repository URL.</p> <pre><code>mlcr push,github,mlperf,inference,submission \\\n   --repo_url=https://github.com/mlcommons/mlperf_inference_submissions_v5.0 \\\n   --commit_message=\"Results on &lt;HW name&gt; added by &lt;Name&gt;\" \\\n   --quiet\n</code></pre> <p>Note: The path to the locally synced submission directory from the output below can be used in the next step by passing it to the <code>--submission_dir</code> argument.  Click to see the sample output <pre><code>[2025-07-23 16:36:56,399 module.py:2197 INFO] - \n\nPath to the locally synced submission directory: mysubmissions/mlperf_submission\n</code></pre> </p> <pre><code>    flowchart LR\n        subgraph SUT1 [Submission Generation SUT1]\n          A[Submission Folder SUT1]\n        end\n        subgraph SUT2 [Submission Generation SUT2]\n          B[Submission Folder SUT2]\n        end\n        subgraph SUT3 [Submission Generation SUT3]\n          C[Submission Folder SUT3]\n        end\n        subgraph SUTN [Submission Generation SUTN]\n          D[Submission Folder SUTN]\n        end\n    SUT2 -- git sync and push --&gt; G[Github Repo]\n    SUT3 -- git sync and push --&gt; G[Github Repo]\n    SUTN -- git sync and push --&gt; G[Github Repo]\n    SUT1 -- git sync and push --&gt; G[Github Repo]\n</code></pre>"},{"location":"submission/submission-cli/#upload-the-final-submission","title":"Upload the final submission","text":"<p>Warning</p> <p>If you are using GitHub for consolidating your results, make sure that you have run the <code>push-to-github</code> command on the same system to ensure results are synced as is on the GitHub repository.</p> <p>Once you have all the results on the system, you can upload them to the MLCommons submission server as follows:</p> via CLIvia Browser <p>You can do the following command which will run the submission checker and upload the results to the MLCommons submission server <pre><code>mlcr run,mlperf,submission,checker,inference \\\n--submitter_id=&lt;&gt; \\\n--submission_dir=&lt;Path to the locally synced submission directory&gt; --quiet\n</code></pre></p> <p>You can do the following command to generate the final submission tar file and then upload to the MLCommons Submission UI.  <pre><code>mlcr run,mlperf,submission,checker,inference \\\n--submission_dir=&lt;Path to the submission folder&gt; \\\n--tar=yes \\\n--submission_tar_file=mysubmission.tar.gz --quiet\n</code></pre></p> <pre><code>        flowchart LR\n            subgraph SUT [Combined Submissions]\n              A[Combined Submission Folder in SUT1]\n            end\n        SUT --&gt; B[Run submission checker]\n        B --&gt; C[Upload to MLC Submission server]\n        C --&gt; D[Receive validation email]</code></pre>"},{"location":"usage/","title":"Using MLC for MLPerf Inference","text":""}]}