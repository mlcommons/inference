The purpose of this test is to ensure that valid inferences are being performed in performance mode. By default,
the inference result that is returned from SUT to Loadgen is not dumped to the accuracy JSON file and thus not
checked for accuracy. In this test, a portion of the results are dumped to the accuracy JSON randomly with some
chosen probability. This accuracy JSON file can then be checked against the accuracy JSON generated in accuracy
mode.

Note that under the MLPerf v0.5 inference rules, certain forms of non-determinism is acceptable, which can cause
inference results to differ across runs. It is foreseeable that the results obtained during the accuracy run
can be different from that obtained during the performance run, which will cause the accuracy checking script
to report failure. Test failure will automatically result in an objection, but the objection can be overturned
by comparing the quality of the results generated in performance mode to that obtained in accuracy mode. This 
can be done by using the accuracy measurement scripts provided as part of the repo to ensure that the
classification accuracy/mAP/BLEU score meets the target. An example is provided for GNMT in the gnmt folder.

If performance with sampling enabled is lower than the submitted performance score, accuracy_log_probability in 
config file can be reduced from 10 (%) to check that performance approaches reported score.

Note that for high-performance machines, a logging probability of 10% can result in a massive number of logged
results. To keep the size of the accuracy log file reasonable, accuracy_log_probability should be reduced to
keep the total number of logged inferences on the order of ~1000 samples for SSD-Large, and ~10000 samples
elsewhere. The probability setting can be calculated as follows:
  accuracy_log_probability = 100% * (1000 or 10000) / <expected number of samples completed during performance run>
Alternatively, if the accuracy log is too large, it can be truncated using the provided truncate_log.sh script
before uploading results.

UPDATE: An alternate script has been provided for reducing the accuracy log. Check truncate_log.py

The mode is set assuming that an accuracy JSON in accuracy mode already exists (i.e. from the submission results)
that can be used to for verification. If not, the mode should be changed from PerformanceOnly (mode=2) to 
AccuracyOnly (mode=1) so that accuracy mode results can be generated.

This test does not use custom dataset or weights.

Instructions

Part I
Run test with provided audit.config.
Note that audit.config must be copied to the directory where the benchmark is being run from.
Verification that audit.config was properly read can be done by checking that the settings in the summary txt 
file matches what is in audit.config.

Part II
The first check is to ensure that accuracy during performance portion of run matches that achieved during
accuracy mode.
  python verify_accuracy.py -a <path to accuracy mode accuracy json> -p <path to performance mode accuracy json>

Expected outcome:
  TEST PASS

Part III
The second check is to ensure that performance with accuracy logging enabled matches submission performance score.
  
  python verify_performance.py -r <submission mlperf_log_summary.txt> -t <this test's mlperf_log_summary.txt>

Expected outcome:
  TEST PASS
  for sufficiently small but non-zero accuracy_log_probability

Part IV
Truncate logs to reduce accuracy log file size in preparation for uploading
  bash ./truncate_log.sh <accuracy_log_file> <number of samples>
