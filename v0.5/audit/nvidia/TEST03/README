The purpose of this test is to ensure that the System-Under-Test (SUT) is not providing precalculated inference
results. The benchmark should be run in Submission mode with the dataset modified using the given scripts. 
The performance must match the submisssion and the accuracy should be within an acceptable range based on 
measurements on the reference implementation.

Instructions

Part I

#Generate custom data for imagenet and coco
Run the script with the path to the original dataset and the new path to store the custom data:
  python modify_image_data.py -d <original data path> -o <new data path> --datatset  [coco|imagenet]
  "original data path" must contain the ImageNet and/or COCO datasets in JPEG format.

#Generate custom data for GNMT
This script assumes you have the original dataset and BPE code files already.  
Please change the ORIGINAL_DATASET and CUSTOM_DATASET_OUTPUT variables in the script to point to where your
original newstest2014 dataset is stored and where you want the custom dataset to be stored respectively.
The script stores intermediate files in OUTPUT_DIR which is set to $PWD/outputs.  This may be cleaned up after
the script completes.  Final dataset will be available in $CUSTOM_DATASET_OUTPUT.
To run the script: 
  ./download_and_modify_gnmt.sh

Part II
Run the benchmark in the same manner as the original submission, once in AccuracyOnly mode and once in
SubmissionOnly mode. Ensure that accuracy.txt is generated along with the other mlperf_log_* logs.  
Note that the expected accuracies are lower than the MLPerf targets so the benchmark may report failure.
This is expected behavior and does not neccessarily mean that the audit has failed. 

Part III
Ensure that performance matches that achieved in the submission run.
  python verify_performance.py -r <submission mlperf_log_summary.txt> -t <this test's mlperf_log_summary.txt>


