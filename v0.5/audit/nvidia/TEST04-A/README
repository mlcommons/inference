The purpose of this test is to ensure that results are not cached on the fly when SUT sees duplicate sample IDs.

By default, QSL loads a subset of the dataset determined by Performance Sample Count (say P) and queries for each scenario are
built using samples from the PerformanceSample implying the same sample can get repeatedly sent to the SUT over 
the test duration.

This test requires measuring & comparing performance of SUT (PerformanceOnly, mode=2) for two audit settings:
	(TEST04-A) Issue P unique samples: In Offline scenario, a single query with samples_per_query equivalent to P unique samples is issued.
		       		    	   In Multi-Stream scenario, test ends after #queries = P/samples_per_query have been issued.
				    	   In Single-Stream/Server scenario test ends after P unique queries have been issued.
	(TEST04-B) Issue same sample P times: In Offline scenario, the same sample is repeated P times to fill the query. This breaks the requirement
                                              of reading contiguous memory locations in Offline mode, but it is normal for an audit test, meant to 
                                              stress the SUT in newer ways, to cause performance degradation.
				              In Multi-Stream scenario, the same query is repeated for #queries (=P/samples_per_query). 
				              In Single-Stream/Server scenario test ends after sending P same queries.
This test is not applicable for:
	(1) GNMT benchmark: Performance of GNMT benchmark is dependant on sample sequence length and hence performance for the two cases mentioned above can differ significantly.
	(2) For Multi-Stream scenario, if samples_per_query >= P: The two cases above are the same and hence do not require testing.

Validation checks:
        TEST04-B should not be significantly faster than TEST04-A in a fair system which does not cache.

This test does not use custom dataset or weights.

Instructions

Part I
	Copy audit.config from TEST04-A folder to the working directory and run test.

Part II 
	Copy audit.config from TEST04-B folder to the working directory and run test.

Part III
	Check the performance reported by TEST04-A matches that of TEST04-B by running the script provided

	python verify_test4_performance.py -u <mlperf_log_summary.txt generated by TEST04-A> -s <mlperf_log_summary.txt generated by TEST04-B>

Expected outcome:
	TEST PASS
